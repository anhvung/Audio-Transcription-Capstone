{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd337ad8",
   "metadata": {
    "id": "fd337ad8"
   },
   "source": [
    "# Whisper Model\n",
    "modified notebook by Sivan Ding (sd5397)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed621d7",
   "metadata": {
    "id": "0ed621d7"
   },
   "source": [
    "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
    "\n",
    "Codes below adpated from [github repo](https://github.com/openai/whisper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeba0ae",
   "metadata": {
    "id": "efeba0ae"
   },
   "source": [
    "## Requirement\n",
    "Python 3.9.9 and PyTorch 1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Uezepu_hzPaE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3475,
     "status": "ok",
     "timestamp": 1665621892920,
     "user": {
      "displayName": "Alexandria Guo",
      "userId": "11397332379485803960"
     },
     "user_tz": 240
    },
    "id": "Uezepu_hzPaE",
    "outputId": "e36536ac-80bc-42ea-db9e-01a5036587ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorednoise in /opt/conda/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: ffmpeg in /opt/conda/lib/python3.7/site-packages (1.4)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.12.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from colorednoise) (1.19.5)\n",
      "Collecting torch==1.12.1\n",
      "  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1->torchaudio) (4.1.1)\n",
      "Installing collected packages: torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.0\n",
      "    Uninstalling torch-1.10.0:\n",
      "      Successfully uninstalled torch-1.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.12.1 which is incompatible.\n",
      "fastai 2.5.3 requires torch<1.11,>=1.7.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1 torchaudio-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install colorednoise ffmpeg torchaudio jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "S-w-qciizG8O",
   "metadata": {
    "id": "S-w-qciizG8O"
   },
   "outputs": [],
   "source": [
    "# Julia's code\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import colorednoise as cn\n",
    "\n",
    "\n",
    "def add_noise(s, sample_rate=16000, noise_percentage_factor = .01, noise_type='white'):\n",
    "    # s: audio input (mono)\n",
    "    # sample rate: sample rate of s\n",
    "    # noise_percentage_factor, percentage scale of added noise added\n",
    "    # type: white, pink, brown\n",
    "\n",
    "    if noise_type == 'white':\n",
    "        beta = 0\n",
    "\n",
    "    elif noise_type == 'pink':\n",
    "        beta = 1\n",
    "\n",
    "    elif noise_type == 'brown':\n",
    "        beta = 2\n",
    "\n",
    "    noise = cn.powerlaw_psd_gaussian(beta, s.size)\n",
    "\n",
    "    noisy_s = s + noise * noise_percentage_factor\n",
    "\n",
    "    # output should be at 16kHz sample rate\n",
    "    if sample_rate != 16000:\n",
    "        noisy_s = librosa.resample(noisy_s, orig_sr = sample_rate, target_sr=16000)\n",
    "\n",
    "    return noisy_s\n",
    "\n",
    "\n",
    "def add_signals(s, back_s, sample_rate=16000, back_sample_rate=16000, noise_db=-12):\n",
    "    # s: audio input (mono)\n",
    "    # back_s: brckgrnd audio\n",
    "    # sample rate: sample rate of s\n",
    "    # noise_db: lower the backgrnd signal by noise_db db\n",
    "\n",
    "\n",
    "    # make sure both signals have same 16kHz sample rate\n",
    "    if sample_rate != 16000:\n",
    "        s = librosa.resample(s, orig_sr=sample_rate, target_sr=16000)\n",
    "\n",
    "    if back_sample_rate != 16000:\n",
    "        back_s = librosa.resample(back_s, orig_sr=back_sample_rate, target_sr=16000)\n",
    "\n",
    "    if s.size > back_s.size:\n",
    "        back_s = librosa.util.pad_center(back_s, size=s.size)\n",
    "\n",
    "    elif s.size < back_s.size:\n",
    "        s = librosa.util.pad_center(s, size=back_s.size)\n",
    "\n",
    "    # lower background signal by noise_db\n",
    "    noise_amp = librosa.db_to_amplitude(noise_db)\n",
    "    lower_back_s = back_s - noise_amp\n",
    "\n",
    "    # add background noise to sound clip\n",
    "    noisy_s = s + back_s\n",
    "\n",
    "    # output should be at 16kHz sample rate\n",
    "    return noisy_s\n",
    "\n",
    "\n",
    "def down_sample(s, sample_rate=16000, output_sr=8000):\n",
    "    # s: audio input (mono)\n",
    "    # sample rate: sample rate of s\n",
    "    # output_sr: output sample rate\n",
    "\n",
    "    # resample to output_sr\n",
    "    resampled_s = librosa.resample(s, orig_sr=sample_rate, target_sr=output_sr)\n",
    "\n",
    "    # then re-resample to 16000\n",
    "    noisy_s = librosa.resample(resampled_s, orig_sr=output_sr, target_sr=16000)\n",
    "\n",
    "    # output should be at 16kHz sample rate\n",
    "    return noisy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58efc4bb",
   "metadata": {
    "id": "58efc4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6567c13b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10865,
     "status": "ok",
     "timestamp": 1665620610648,
     "user": {
      "displayName": "Alexandria Guo",
      "userId": "11397332379485803960"
     },
     "user_tz": 240
    },
    "id": "6567c13b",
    "outputId": "0a48c983-fadb-4bfa-cc3a-b95085ced5bf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-o9wbjgup\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-o9wbjgup\n",
      "  Resolved https://github.com/openai/whisper.git to commit d18e9ea5dd2ca57c697e8e55f9e654f06ede25d0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from whisper==1.0) (1.19.5)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from whisper==1.0) (1.12.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from whisper==1.0) (4.62.3)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from whisper==1.0) (8.14.0)\n",
      "Requirement already satisfied: transformers>=4.19.0 in /opt/conda/lib/python3.7/site-packages (from whisper==1.0) (4.23.1)\n",
      "Requirement already satisfied: ffmpeg-python==0.2.0 in /opt/conda/lib/python3.7/site-packages (from whisper==1.0) (0.2.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.18.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (2.27.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (4.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (2022.9.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (0.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.19.0->whisper==1.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->whisper==1.0) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.19.0->whisper==1.0) (3.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29aeb2",
   "metadata": {
    "id": "ab29aeb2"
   },
   "source": [
    "# Base model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ef685",
   "metadata": {
    "id": "138ef685"
   },
   "source": [
    "## English-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21b5803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22815,
     "status": "ok",
     "timestamp": 1665621013289,
     "user": {
      "displayName": "Alexandria Guo",
      "userId": "11397332379485803960"
     },
     "user_tz": 240
    },
    "id": "a21b5803",
    "outputId": "521ea7ec-507d-44f1-afc8-fbfb582f28da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "#audio = whisper.load_audio(\"audio.mp3\")\n",
    "#audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "#mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "#_, probs = model.detect_language(mel)\n",
    "#print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "#options = whisper.DecodingOptions(fp16 = False)\n",
    "#result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "#print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6f544",
   "metadata": {
    "id": "0ef6f544"
   },
   "source": [
    "### evaluation on LibriSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0adaf9d",
   "metadata": {
    "id": "d0adaf9d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcdcb98f4f5475eb6ff095f0a63995e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "# installing takes ~30 seconds\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
    "    It will drop the last few seconds of a very small portion of the utterances.\n",
    "    MODIFIED: for white noise \n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test-clean\", device=DEVICE):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "            root=os.path.expanduser(\"~/.cache\"),\n",
    "            url=split,\n",
    "            download=True,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
    "        assert sample_rate == 16000\n",
    "\n",
    "        audio_clean = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
    "        mel_c = whisper.log_mel_spectrogram(audio_clean)\n",
    "\n",
    "        audio_noise = add_noise(audio.numpy(), sample_rate=sample_rate) # default white noise\n",
    "        audio_noise = torch.from_numpy(audio_noise.astype('float32'))\n",
    "        audio_noise = whisper.pad_or_trim(audio_noise.flatten()).to(self.device)\n",
    "        mel_n = whisper.log_mel_spectrogram(audio_noise)\n",
    "\n",
    "        audio_ds = down_sample(audio.numpy().flatten(), sample_rate=sample_rate) # default params\n",
    "        audio_ds = torch.from_numpy(audio_ds.astype('float32'))\n",
    "        audio_ds = whisper.pad_or_trim(audio_ds.flatten()).to(self.device)\n",
    "        mel_d = whisper.log_mel_spectrogram(audio_ds)\n",
    "        \n",
    "        return (mel_c, mel_n, mel_d, text)\n",
    "    \n",
    "dataset = LibriSpeech(\"test-clean\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577268b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1878,
     "status": "ok",
     "timestamp": 1665627120687,
     "user": {
      "displayName": "Alexandria Guo",
      "userId": "11397332379485803960"
     },
     "user_tz": 240
    },
    "id": "577268b3",
    "outputId": "9acba9a1-b834-40aa-8812-dd76a5970329"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:07<00:00, 20.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is English-only and has 71,825,408 parameters.\n"
     ]
    }
   ],
   "source": [
    "# load base model and review\n",
    "model = whisper.load_model(\"base.en\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32e7f68",
   "metadata": {
    "id": "d32e7f68"
   },
   "outputs": [],
   "source": [
    "# predict without timestamps for short-form transcription\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True, fp16 = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae58fd06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "723136acded84f0b9732b36686ae499d",
      "48092cbcdba545c78981b9a98f18c952",
      "a4f9e4c96272422e9a25e7057dbcef50",
      "f36a2a32b1904fe0a1fa6ec6f71f2959",
      "f8fa0d556df149b580df93230603ca18",
      "9ce122e66b33457b8c94de6a0a08a2dd",
      "53e2a998d23247e68676de5ba64c5d8a",
      "39c4af0ff4b542e8a9bfdcc9bda65a1e",
      "ed10533543dd4f0994088f97899aeb50",
      "ec3ba4a418da4fbbaf7ed341b76a4494",
      "488cf9010cf940bcaacf00bc25827357"
     ]
    },
    "id": "ae58fd06",
    "outputId": "6ae8dfa7-be25-4221-ea97-01261536108f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68aae7ad1dfd43cb93ed18097c3fc3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# base model + 2619 samples will take 12 hours....\n",
    "hypotheses_c = []\n",
    "hypotheses_n = []\n",
    "hypotheses_d = []\n",
    "references = []\n",
    "\n",
    "for i, (mels_c, mels_n, mels_d, texts) in enumerate(tqdm(loader)):\n",
    "    results_c = model.decode(mels_c, options)\n",
    "    hypotheses_c.extend([result.text for result in results_c])\n",
    "    results_n = model.decode(mels_n, options)\n",
    "    hypotheses_n.extend([result.text for result in results_n])\n",
    "    results_d = model.decode(mels_d, options)\n",
    "    hypotheses_d.extend([result.text for result in results_d])\n",
    "    \n",
    "    references.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abeb9a22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "abeb9a22",
    "outputId": "1c72d25a-3e87-4c60-a8d1-1da9d2f73bd7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect on testing results\n",
    "data_clean = pd.DataFrame(dict(hypothesis=hypotheses_c, reference=references))\n",
    "data_noise = pd.DataFrame(dict(hypothesis=hypotheses_n, reference=references))\n",
    "data_downs = pd.DataFrame(dict(hypothesis=hypotheses_d, reference=references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec43b4da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "ec43b4da",
    "outputId": "f2089bc9-f535-441e-f192-26e52ae82b5e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>reference</th>\n",
       "      <th>hypothesis_clean</th>\n",
       "      <th>reference_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He hoped there would be stew for dinner, turni...</td>\n",
       "      <td>HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...</td>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuffered into you, his belly counseled him.</td>\n",
       "      <td>STUFF IT INTO YOU HIS BELLY COUNSELLED HIM</td>\n",
       "      <td>stuffered into you his belly counseled him</td>\n",
       "      <td>stuff it into you his belly counseled him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After early nightfall the yellow lamps would l...</td>\n",
       "      <td>AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...</td>\n",
       "      <td>after early nightfall the yellow lamps would l...</td>\n",
       "      <td>after early nightfall the yellow lamps would l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello Bertie, any good in your mind?</td>\n",
       "      <td>HELLO BERTIE ANY GOOD IN YOUR MIND</td>\n",
       "      <td>hello bertie any good in your mind</td>\n",
       "      <td>hello bertie any good in your mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number 10. Fresh Nelly is waiting on you. Good...</td>\n",
       "      <td>NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...</td>\n",
       "      <td>number 10 fresh nelly is waiting on you good n...</td>\n",
       "      <td>number 10 fresh nelly is waiting on you good n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>Oh, to shoot my soul's full meaning into futur...</td>\n",
       "      <td>OH TO SHOOT MY SOUL'S FULL MEANING INTO FUTURE...</td>\n",
       "      <td>0 to shoot my soul is full meaning into future...</td>\n",
       "      <td>0 to shoot my soul is full meaning into future...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>Then I, long tried by natural ills, received t...</td>\n",
       "      <td>THEN I LONG TRIED BY NATURAL ILLS RECEIVED THE...</td>\n",
       "      <td>then i long tried by natural ills received the...</td>\n",
       "      <td>then i long tried by natural ills received the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>I love thee freely as men strive for right. I ...</td>\n",
       "      <td>I LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I L...</td>\n",
       "      <td>i love thee freely as men strive for right i l...</td>\n",
       "      <td>i love thee freely as men strive for right i l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>I love thee with the passion put to use, in my...</td>\n",
       "      <td>I LOVE THEE WITH THE PASSION PUT TO USE IN MY ...</td>\n",
       "      <td>i love thee with the passion put to use in my ...</td>\n",
       "      <td>i love thee with the passion put to use in my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>I love thee with the love I seemed to lose wit...</td>\n",
       "      <td>I LOVE THEE WITH A LOVE I SEEMED TO LOSE WITH ...</td>\n",
       "      <td>i love thee with the love i seemed to lose wit...</td>\n",
       "      <td>i love thee with a love i seemed to lose with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2620 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             hypothesis  \\\n",
       "0     He hoped there would be stew for dinner, turni...   \n",
       "1          Stuffered into you, his belly counseled him.   \n",
       "2     After early nightfall the yellow lamps would l...   \n",
       "3                  Hello Bertie, any good in your mind?   \n",
       "4     Number 10. Fresh Nelly is waiting on you. Good...   \n",
       "...                                                 ...   \n",
       "2615  Oh, to shoot my soul's full meaning into futur...   \n",
       "2616  Then I, long tried by natural ills, received t...   \n",
       "2617  I love thee freely as men strive for right. I ...   \n",
       "2618  I love thee with the passion put to use, in my...   \n",
       "2619  I love thee with the love I seemed to lose wit...   \n",
       "\n",
       "                                              reference  \\\n",
       "0     HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...   \n",
       "1            STUFF IT INTO YOU HIS BELLY COUNSELLED HIM   \n",
       "2     AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...   \n",
       "3                    HELLO BERTIE ANY GOOD IN YOUR MIND   \n",
       "4     NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...   \n",
       "...                                                 ...   \n",
       "2615  OH TO SHOOT MY SOUL'S FULL MEANING INTO FUTURE...   \n",
       "2616  THEN I LONG TRIED BY NATURAL ILLS RECEIVED THE...   \n",
       "2617  I LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I L...   \n",
       "2618  I LOVE THEE WITH THE PASSION PUT TO USE IN MY ...   \n",
       "2619  I LOVE THEE WITH A LOVE I SEEMED TO LOSE WITH ...   \n",
       "\n",
       "                                       hypothesis_clean  \\\n",
       "0     he hoped there would be stew for dinner turnip...   \n",
       "1            stuffered into you his belly counseled him   \n",
       "2     after early nightfall the yellow lamps would l...   \n",
       "3                    hello bertie any good in your mind   \n",
       "4     number 10 fresh nelly is waiting on you good n...   \n",
       "...                                                 ...   \n",
       "2615  0 to shoot my soul is full meaning into future...   \n",
       "2616  then i long tried by natural ills received the...   \n",
       "2617  i love thee freely as men strive for right i l...   \n",
       "2618  i love thee with the passion put to use in my ...   \n",
       "2619  i love thee with the love i seemed to lose wit...   \n",
       "\n",
       "                                        reference_clean  \n",
       "0     he hoped there would be stew for dinner turnip...  \n",
       "1             stuff it into you his belly counseled him  \n",
       "2     after early nightfall the yellow lamps would l...  \n",
       "3                    hello bertie any good in your mind  \n",
       "4     number 10 fresh nelly is waiting on you good n...  \n",
       "...                                                 ...  \n",
       "2615  0 to shoot my soul is full meaning into future...  \n",
       "2616  then i long tried by natural ills received the...  \n",
       "2617  i love thee freely as men strive for right i l...  \n",
       "2618  i love thee with the passion put to use in my ...  \n",
       "2619  i love thee with a love i seemed to lose with ...  \n",
       "\n",
       "[2620 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jiwer\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "\n",
    "def clean_text(normalizer, pred_dict):\n",
    "  pred_dict[\"hypothesis_clean\"] = [normalizer(text) for text in pred_dict[\"hypothesis\"]]\n",
    "  pred_dict[\"reference_clean\"] = [normalizer(text) for text in pred_dict[\"reference\"]]\n",
    "  return(pred_dict)\n",
    "\n",
    "normalizer = EnglishTextNormalizer()\n",
    "data_clean = clean_text(normalizer, data_clean)\n",
    "data_noise = clean_text(normalizer, data_noise)\n",
    "data_downs = clean_text(normalizer, data_downs)\n",
    "\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41514e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41514e8f",
    "outputId": "7b3dbe7c-a37e-4a07-a50a-b27d5f88b68f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean WER: 4.27 %\n",
      "White noise WER: 6.22 %\n",
      "Downsampled WER: 4.79 %\n"
     ]
    }
   ],
   "source": [
    "wer_c = jiwer.wer(list(data_clean[\"reference_clean\"]), list(data_clean[\"hypothesis_clean\"]))\n",
    "wer_n = jiwer.wer(list(data_noise[\"reference_clean\"]), list(data_noise[\"hypothesis_clean\"]))\n",
    "wer_d = jiwer.wer(list(data_downs[\"reference_clean\"]), list(data_downs[\"hypothesis_clean\"]))\n",
    "\n",
    "print(f\"Clean WER: {wer_c * 100:.2f} %\")\n",
    "print(f\"White noise WER: {wer_n * 100:.2f} %\")\n",
    "print(f\"Downsampled WER: {wer_d * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "39c4af0ff4b542e8a9bfdcc9bda65a1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48092cbcdba545c78981b9a98f18c952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ce122e66b33457b8c94de6a0a08a2dd",
      "placeholder": "​",
      "style": "IPY_MODEL_53e2a998d23247e68676de5ba64c5d8a",
      "value": "  0%"
     }
    },
    "488cf9010cf940bcaacf00bc25827357": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53e2a998d23247e68676de5ba64c5d8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "723136acded84f0b9732b36686ae499d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48092cbcdba545c78981b9a98f18c952",
       "IPY_MODEL_a4f9e4c96272422e9a25e7057dbcef50",
       "IPY_MODEL_f36a2a32b1904fe0a1fa6ec6f71f2959"
      ],
      "layout": "IPY_MODEL_f8fa0d556df149b580df93230603ca18"
     }
    },
    "9ce122e66b33457b8c94de6a0a08a2dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4f9e4c96272422e9a25e7057dbcef50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39c4af0ff4b542e8a9bfdcc9bda65a1e",
      "max": 164,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed10533543dd4f0994088f97899aeb50",
      "value": 0
     }
    },
    "ec3ba4a418da4fbbaf7ed341b76a4494": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed10533543dd4f0994088f97899aeb50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f36a2a32b1904fe0a1fa6ec6f71f2959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec3ba4a418da4fbbaf7ed341b76a4494",
      "placeholder": "​",
      "style": "IPY_MODEL_488cf9010cf940bcaacf00bc25827357",
      "value": " 0/164 [00:00&lt;?, ?it/s]"
     }
    },
    "f8fa0d556df149b580df93230603ca18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
