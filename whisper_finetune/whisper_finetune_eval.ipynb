{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate finetuned Whisper on Fleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reproduce eval demo on Chinese to check validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "check text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "import utils\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# yue_hant_hk for Cantonese and cmn_hans_cn for Chinese\n",
    "dataset = load_dataset(\"google/fleurs\", \"cmn_hans_cn\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "    num_rows: 945\n",
      "})\n",
      "{'audio': {'path': '11338778690242874435.wav', 'array': array([ 4.17232513e-07, -5.36441803e-07,  5.96046448e-07, ...,\n",
      "       -1.31678581e-03, -1.24770403e-03, -1.29789114e-03]), 'sampling_rate': 16000}, 'transcription': '1940 年 8 月 15 日 盟 军 攻 入 法 国 南 部 这 次 进 攻 被 称 为 龙 骑 兵 行 动', 'raw_transcription': '1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "# print(dataset[0])\n",
    "dataset = dataset.remove_columns(['id', 'num_samples', 'path', 'gender', 'lang_id', 'language', 'lang_group_id'])\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Chinese\", task=\"transcribe\")\n",
    "# processor\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Chinese\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。\n",
      "Decoded w/ special:    <|startoftranscript|><|zh|><|transcribe|><|notimestamps|>1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。<|endoftext|>\n",
      "Decoded w/out special: 1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = dataset[0][\"raw_transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check base model on Chinese (simplified and cantonese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b079a7622e45f8a42007ba2fd7114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/945 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "# using \"zh\" in model => traditional chinese whereas Fleurs uses simplified Chinese for cmn_hans_cn, and yue_hant_hk\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"zh\", task=\"transcribe\")\n",
    "result = dataset.map(utils.map_to_pred, processor, model, \"zh\", batched=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.38239503090460675\n"
     ]
    }
   ],
   "source": [
    "print(\"WER:\", wer(result[\"ground_truth\"], result[\"prediction\"]))  # (truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': None, 'array': array([ 0.00000000e+00, -3.05175781e-05,  0.00000000e+00, ...,\n",
      "       -1.34277344e-03, -1.25122070e-03, -1.31225586e-03]), 'sampling_rate': 16000}, 'transcription': '1940 年 8 月 15 日 盟 军 攻 入 法 国 南 部 这 次 进 攻 被 称 为 龙 骑 兵 行 动', 'raw_transcription': '1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。', 'prediction': '1940 年 8 月 15 日 蒙 軍 公 路 法 國 南 部 這 次 進 攻 被 稱 為 龍 旗 冰 晴 洞', 'ground_truth': '1940 年 8 月 15 日 盟 军 攻 入 法 国 南 部 这 次 进 攻 被 称 为 龙 骑 兵 行 动'}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/yue_hant_hk/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# yue_hant_hk for Cantonese and cmn_hans_cn for Chinese\n",
    "dataset = load_dataset(\"google/fleurs\", \"yue_hant_hk\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "    num_rows: 819\n",
      "})\n",
      "{'audio': {'path': '4705644860951940224.wav', 'array': array([ 0.        ,  0.        ,  0.        , ...,  0.00025684,\n",
      "       -0.00013679, -0.00051916]), 'sampling_rate': 16000}, 'transcription': '仍 有 許 多 當 時 在 此 的 男 女 存 活 了 下 來 還 有 更 多 人 的 摯 愛 在 此 被 殺 害 或 勞 動 至 死 不 管 是 不 是 猶 太'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "# print(dataset[0])\n",
    "dataset = dataset.remove_columns(\n",
    "    ['id', 'num_samples', 'path', 'gender', 'lang_id', 'language', 'lang_group_id', 'raw_transcription'])\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387e40a74e2146888291eae63678aa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/819 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 57s, sys: 14.3 s, total: 7min 11s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "# using \"zh\" in model => traditional chinese whereas Fleurs uses simplified Chinese for cmn_hans_cn, and yue_hant_hk\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"zh\", task=\"transcribe\")\n",
    "result = dataset.map(utils.map_to_pred, batched=True, \"zh\", remove_columns=['audio'], batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.4356259136449531\n"
     ]
    }
   ],
   "source": [
    "print(\"WER:\", wer(result[\"transcription\"], result[\"prediction\"]))  # (truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': '仍 有 許 多 當 時 在 此 的 男 女 存 活 了 下 來 還 有 更 多 人 的 摯 愛 在 此 被 殺 害 或 勞 動 至 死 不 管 是 不 是 猶 太', 'prediction': '仍 有 許 多 當 時 在 此 的 男 女 徐 樂 了 下 來 還 有 更 多 人 的 自 愛 在 此 被 殺 害 惑 努 動 之 死 不 過 是 不 事 由 太 '}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reproduce all reported results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 03:25:37.658265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 03:25:38.574598: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 03:25:40.257519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-18 03:25:40.257685: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-18 03:25:40.257699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperTokenizer\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !gsutil -m cp -n -r gs://capstone_datasets/fleurs/preprocess/fl_ch_features /home/sivan/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"ko_kr\", \"he_il\", \"te_in\", \"cmn_hans_cn\"]  # token for Fleurs dataset\n",
    "codes = {\"te_in\": 'te', \"ko_kr\": 'ko', \"he_il\": 'iw',\n",
    "         \"cmn_hans_cn\": 'zh'}  # token for normalizer and processor decoder\n",
    "languages = {\"te_in\": 'Telugu', \"ko_kr\": 'Korean', \"he_il\": 'Hebrew', \"cmn_hans_cn\": 'Chinese'}  # token for Whisper processor and tokenizer\n",
    "best = {\"te_in\": 1000, \"ko_kr\": 2000, \"he_il\": 1000,\n",
    "        \"cmn_hans_cn\": 1000}  # best checkpoint for each model\n",
    "tests = {}\n",
    "preprocess = {}\n",
    "models = {}\n",
    "collators = {}\n",
    "processors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/ko_kr/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n",
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/he_il/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n",
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n",
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/cmn_hans_cn/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    # load dataset from disk\n",
    "    preprocess[lang] = load_from_disk(f\"/home/sivan/datasets/fl_{lang}_features\")\n",
    "    tests[lang] = load_dataset(\"google/fleurs\", lang, split='test').remove_columns(['id', 'num_samples', 'path', 'gender', 'lang_id', 'language', 'lang_group_id'])\n",
    "\n",
    "\n",
    "    # print(fleurs[lang])\n",
    "\n",
    "    # load whisper processor\n",
    "    processors[lang] = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=languages[lang],\n",
    "                                                        task=\"transcribe\")\n",
    "\n",
    "    # initialize data collator\n",
    "    collators[lang] = utils.DataCollatorSpeechSeq2SeqWithPadding(processor=processors[lang])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 25\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# universal training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/sivan\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    metric_for_best_model=\"wer\",\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    disable_tqdm=False,  # set false to see progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for ko_kr starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/vocab.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file merges.txt from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/merges.txt\n",
      "loading file normalizer.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/normalizer.json\n",
      "loading file added_tokens.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/tokenizer_config.json\n",
      "Adding <|endoftext|> to the vocabulary\n",
      "Adding <|startoftranscript|> to the vocabulary\n",
      "Adding <|en|> to the vocabulary\n",
      "Adding <|zh|> to the vocabulary\n",
      "Adding <|de|> to the vocabulary\n",
      "Adding <|es|> to the vocabulary\n",
      "Adding <|ru|> to the vocabulary\n",
      "Adding <|ko|> to the vocabulary\n",
      "Adding <|fr|> to the vocabulary\n",
      "Adding <|ja|> to the vocabulary\n",
      "Adding <|pt|> to the vocabulary\n",
      "Adding <|tr|> to the vocabulary\n",
      "Adding <|pl|> to the vocabulary\n",
      "Adding <|ca|> to the vocabulary\n",
      "Adding <|nl|> to the vocabulary\n",
      "Adding <|ar|> to the vocabulary\n",
      "Adding <|sv|> to the vocabulary\n",
      "Adding <|it|> to the vocabulary\n",
      "Adding <|id|> to the vocabulary\n",
      "Adding <|hi|> to the vocabulary\n",
      "Adding <|fi|> to the vocabulary\n",
      "Adding <|vi|> to the vocabulary\n",
      "Adding <|iw|> to the vocabulary\n",
      "Adding <|uk|> to the vocabulary\n",
      "Adding <|el|> to the vocabulary\n",
      "Adding <|ms|> to the vocabulary\n",
      "Adding <|cs|> to the vocabulary\n",
      "Adding <|ro|> to the vocabulary\n",
      "Adding <|da|> to the vocabulary\n",
      "Adding <|hu|> to the vocabulary\n",
      "Adding <|ta|> to the vocabulary\n",
      "Adding <|no|> to the vocabulary\n",
      "Adding <|th|> to the vocabulary\n",
      "Adding <|ur|> to the vocabulary\n",
      "Adding <|hr|> to the vocabulary\n",
      "Adding <|bg|> to the vocabulary\n",
      "Adding <|lt|> to the vocabulary\n",
      "Adding <|la|> to the vocabulary\n",
      "Adding <|mi|> to the vocabulary\n",
      "Adding <|ml|> to the vocabulary\n",
      "Adding <|cy|> to the vocabulary\n",
      "Adding <|sk|> to the vocabulary\n",
      "Adding <|te|> to the vocabulary\n",
      "Adding <|fa|> to the vocabulary\n",
      "Adding <|lv|> to the vocabulary\n",
      "Adding <|bn|> to the vocabulary\n",
      "Adding <|sr|> to the vocabulary\n",
      "Adding <|az|> to the vocabulary\n",
      "Adding <|sl|> to the vocabulary\n",
      "Adding <|kn|> to the vocabulary\n",
      "Adding <|et|> to the vocabulary\n",
      "Adding <|mk|> to the vocabulary\n",
      "Adding <|br|> to the vocabulary\n",
      "Adding <|eu|> to the vocabulary\n",
      "Adding <|is|> to the vocabulary\n",
      "Adding <|hy|> to the vocabulary\n",
      "Adding <|ne|> to the vocabulary\n",
      "Adding <|mn|> to the vocabulary\n",
      "Adding <|bs|> to the vocabulary\n",
      "Adding <|kk|> to the vocabulary\n",
      "Adding <|sq|> to the vocabulary\n",
      "Adding <|sw|> to the vocabulary\n",
      "Adding <|gl|> to the vocabulary\n",
      "Adding <|mr|> to the vocabulary\n",
      "Adding <|pa|> to the vocabulary\n",
      "Adding <|si|> to the vocabulary\n",
      "Adding <|km|> to the vocabulary\n",
      "Adding <|sn|> to the vocabulary\n",
      "Adding <|yo|> to the vocabulary\n",
      "Adding <|so|> to the vocabulary\n",
      "Adding <|af|> to the vocabulary\n",
      "Adding <|oc|> to the vocabulary\n",
      "Adding <|ka|> to the vocabulary\n",
      "Adding <|be|> to the vocabulary\n",
      "Adding <|tg|> to the vocabulary\n",
      "Adding <|sd|> to the vocabulary\n",
      "Adding <|gu|> to the vocabulary\n",
      "Adding <|am|> to the vocabulary\n",
      "Adding <|yi|> to the vocabulary\n",
      "Adding <|lo|> to the vocabulary\n",
      "Adding <|uz|> to the vocabulary\n",
      "Adding <|fo|> to the vocabulary\n",
      "Adding <|ht|> to the vocabulary\n",
      "Adding <|ps|> to the vocabulary\n",
      "Adding <|tk|> to the vocabulary\n",
      "Adding <|nn|> to the vocabulary\n",
      "Adding <|mt|> to the vocabulary\n",
      "Adding <|sa|> to the vocabulary\n",
      "Adding <|lb|> to the vocabulary\n",
      "Adding <|my|> to the vocabulary\n",
      "Adding <|bo|> to the vocabulary\n",
      "Adding <|tl|> to the vocabulary\n",
      "Adding <|mg|> to the vocabulary\n",
      "Adding <|as|> to the vocabulary\n",
      "Adding <|tt|> to the vocabulary\n",
      "Adding <|haw|> to the vocabulary\n",
      "Adding <|ln|> to the vocabulary\n",
      "Adding <|ha|> to the vocabulary\n",
      "Adding <|ba|> to the vocabulary\n",
      "Adding <|jw|> to the vocabulary\n",
      "Adding <|su|> to the vocabulary\n",
      "Adding <|translate|> to the vocabulary\n",
      "Adding <|transcribe|> to the vocabulary\n",
      "Adding <|startoflm|> to the vocabulary\n",
      "Adding <|startofprev|> to the vocabulary\n",
      "Adding <|nocaptions|> to the vocabulary\n",
      "Adding <|notimestamps|> to the vocabulary\n",
      "***** Running Prediction *****\n",
      "  Num examples = 382\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/12 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 피라미드 사운드와 광선쇼는 이 지역에서 어린이들의 흥미를 가장 많이 끌어드리는 것들 중 하나입니다.', ' 더 전통적인 교회는 부활절주간에 토요일 밤에 부활 처리하기를 열며 여기에서 신두들이 종종 자정의 주의 부활을 축하합니다.', ' 산을 둘러싸고 있는 맑고 아름다운 하늘 바뀌는 보이지 않는다. 이 동구란에서는 바깥 세상이 보이지도 들리지도 않는다.', ' 장면들이 피라미드들 위에 비쳤고 다른 피라미드들에는 불이 밝혀졌다.', ' 뇌 병리와 행동 사이에 상관 관계가 과학자들의 영구를 독부습니다.'] ['피라미드 사운드와 광선 쇼는 이 지역에서 어린이들의 흥미를 가장 많이 끌어들이는 것들 중 하나입니다.', '더 전통적인 교회는 부활절 주간의 토요일 밤에 부활 철야제를 열며, 여기에서 신도들이 종종 자정에 주의 부활을 축하합니다.', '산을 둘러싸고 있는 맑고 아름다운 하늘밖에는 보이지 않는다. 이 동굴 안에서는 바깥세상이 보이지도, 들리지도 않는다.', '장면들이 피라미드들 위에 비쳤고 다른 피라미드들에는 불이 밝혀졌다.', '뇌 병리와 행동 사이의 상관관계가 과학자들의 연구를 돕습니다.']\n",
      "***** test2 metrics *****\n",
      "  test_loss               =     1.2711\n",
      "  test_runtime            = 0:01:48.37\n",
      "  test_samples_per_second =      3.525\n",
      "  test_steps_per_second   =      0.111\n",
      "  test_wer                =    32.8191\n",
      "  test_wer_norm           =    29.1801\n",
      "Evaluation for he_il starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/vocab.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file merges.txt from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/merges.txt\n",
      "loading file normalizer.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/normalizer.json\n",
      "loading file added_tokens.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/tokenizer_config.json\n",
      "Adding <|endoftext|> to the vocabulary\n",
      "Adding <|startoftranscript|> to the vocabulary\n",
      "Adding <|en|> to the vocabulary\n",
      "Adding <|zh|> to the vocabulary\n",
      "Adding <|de|> to the vocabulary\n",
      "Adding <|es|> to the vocabulary\n",
      "Adding <|ru|> to the vocabulary\n",
      "Adding <|ko|> to the vocabulary\n",
      "Adding <|fr|> to the vocabulary\n",
      "Adding <|ja|> to the vocabulary\n",
      "Adding <|pt|> to the vocabulary\n",
      "Adding <|tr|> to the vocabulary\n",
      "Adding <|pl|> to the vocabulary\n",
      "Adding <|ca|> to the vocabulary\n",
      "Adding <|nl|> to the vocabulary\n",
      "Adding <|ar|> to the vocabulary\n",
      "Adding <|sv|> to the vocabulary\n",
      "Adding <|it|> to the vocabulary\n",
      "Adding <|id|> to the vocabulary\n",
      "Adding <|hi|> to the vocabulary\n",
      "Adding <|fi|> to the vocabulary\n",
      "Adding <|vi|> to the vocabulary\n",
      "Adding <|iw|> to the vocabulary\n",
      "Adding <|uk|> to the vocabulary\n",
      "Adding <|el|> to the vocabulary\n",
      "Adding <|ms|> to the vocabulary\n",
      "Adding <|cs|> to the vocabulary\n",
      "Adding <|ro|> to the vocabulary\n",
      "Adding <|da|> to the vocabulary\n",
      "Adding <|hu|> to the vocabulary\n",
      "Adding <|ta|> to the vocabulary\n",
      "Adding <|no|> to the vocabulary\n",
      "Adding <|th|> to the vocabulary\n",
      "Adding <|ur|> to the vocabulary\n",
      "Adding <|hr|> to the vocabulary\n",
      "Adding <|bg|> to the vocabulary\n",
      "Adding <|lt|> to the vocabulary\n",
      "Adding <|la|> to the vocabulary\n",
      "Adding <|mi|> to the vocabulary\n",
      "Adding <|ml|> to the vocabulary\n",
      "Adding <|cy|> to the vocabulary\n",
      "Adding <|sk|> to the vocabulary\n",
      "Adding <|te|> to the vocabulary\n",
      "Adding <|fa|> to the vocabulary\n",
      "Adding <|lv|> to the vocabulary\n",
      "Adding <|bn|> to the vocabulary\n",
      "Adding <|sr|> to the vocabulary\n",
      "Adding <|az|> to the vocabulary\n",
      "Adding <|sl|> to the vocabulary\n",
      "Adding <|kn|> to the vocabulary\n",
      "Adding <|et|> to the vocabulary\n",
      "Adding <|mk|> to the vocabulary\n",
      "Adding <|br|> to the vocabulary\n",
      "Adding <|eu|> to the vocabulary\n",
      "Adding <|is|> to the vocabulary\n",
      "Adding <|hy|> to the vocabulary\n",
      "Adding <|ne|> to the vocabulary\n",
      "Adding <|mn|> to the vocabulary\n",
      "Adding <|bs|> to the vocabulary\n",
      "Adding <|kk|> to the vocabulary\n",
      "Adding <|sq|> to the vocabulary\n",
      "Adding <|sw|> to the vocabulary\n",
      "Adding <|gl|> to the vocabulary\n",
      "Adding <|mr|> to the vocabulary\n",
      "Adding <|pa|> to the vocabulary\n",
      "Adding <|si|> to the vocabulary\n",
      "Adding <|km|> to the vocabulary\n",
      "Adding <|sn|> to the vocabulary\n",
      "Adding <|yo|> to the vocabulary\n",
      "Adding <|so|> to the vocabulary\n",
      "Adding <|af|> to the vocabulary\n",
      "Adding <|oc|> to the vocabulary\n",
      "Adding <|ka|> to the vocabulary\n",
      "Adding <|be|> to the vocabulary\n",
      "Adding <|tg|> to the vocabulary\n",
      "Adding <|sd|> to the vocabulary\n",
      "Adding <|gu|> to the vocabulary\n",
      "Adding <|am|> to the vocabulary\n",
      "Adding <|yi|> to the vocabulary\n",
      "Adding <|lo|> to the vocabulary\n",
      "Adding <|uz|> to the vocabulary\n",
      "Adding <|fo|> to the vocabulary\n",
      "Adding <|ht|> to the vocabulary\n",
      "Adding <|ps|> to the vocabulary\n",
      "Adding <|tk|> to the vocabulary\n",
      "Adding <|nn|> to the vocabulary\n",
      "Adding <|mt|> to the vocabulary\n",
      "Adding <|sa|> to the vocabulary\n",
      "Adding <|lb|> to the vocabulary\n",
      "Adding <|my|> to the vocabulary\n",
      "Adding <|bo|> to the vocabulary\n",
      "Adding <|tl|> to the vocabulary\n",
      "Adding <|mg|> to the vocabulary\n",
      "Adding <|as|> to the vocabulary\n",
      "Adding <|tt|> to the vocabulary\n",
      "Adding <|haw|> to the vocabulary\n",
      "Adding <|ln|> to the vocabulary\n",
      "Adding <|ha|> to the vocabulary\n",
      "Adding <|ba|> to the vocabulary\n",
      "Adding <|jw|> to the vocabulary\n",
      "Adding <|su|> to the vocabulary\n",
      "Adding <|translate|> to the vocabulary\n",
      "Adding <|transcribe|> to the vocabulary\n",
      "Adding <|startoflm|> to the vocabulary\n",
      "Adding <|startofprev|> to the vocabulary\n",
      "Adding <|nocaptions|> to the vocabulary\n",
      "Adding <|notimestamps|> to the vocabulary\n",
      "***** Running Prediction *****\n",
      "  Num examples = 792\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/25 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' בדרך כלל תמיד שום עם את כלות התיירים ועמוקרים סיבור הכל והאור הוא ממש כמו סיבורים', ' זודר חשובה להבדיד במספר פעלים והצמין.', ' הגובה מנימה למטחת לגשר הוא 50% הבנייה ה-20% באוגוס 2.1, וואונפתח לתנוע רק במרצ 2.2.0.', ' אז מה רעדיו כנדלוואל לבטו, שפן בו גבאדשן כמו חן לקסיג הציג את.', ' אבל הרבה דברים את זה ציפורים מדליים כמו דינוזווי'] ['בדרך כלל תמיד שומעים את קולות התיירים והמוכרים. סיפור הקול והאור הוא ממש כמו ספר סיפורים.', 'זו דרך חשובה להבדיל בין מספר פעלים ועצמים.', 'הגובה המינימלי מתחת לגשר הוא 15 מטר. הבנייה הסתיימה באוגוסט 2011, והוא נפתח לתנועה רק במרץ 2017.', \"כמו כן, לאקה סינג הציג את chhappan bhog bhajan. הזמר ראג'ו קנדלוואל ליווה אותו.\", 'אבל הרבה דברים אצל ציפורים עדיין נראים כמו דינוזאורים.']\n",
      "***** test2 metrics *****\n",
      "  test_loss               =     1.7893\n",
      "  test_runtime            = 0:05:59.23\n",
      "  test_samples_per_second =      2.205\n",
      "  test_steps_per_second   =       0.07\n",
      "  test_wer                =    72.1853\n",
      "  test_wer_norm           =    69.7215\n",
      "Evaluation for te_in starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/vocab.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file merges.txt from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/merges.txt\n",
      "loading file normalizer.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/normalizer.json\n",
      "loading file added_tokens.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/tokenizer_config.json\n",
      "Adding <|endoftext|> to the vocabulary\n",
      "Adding <|startoftranscript|> to the vocabulary\n",
      "Adding <|en|> to the vocabulary\n",
      "Adding <|zh|> to the vocabulary\n",
      "Adding <|de|> to the vocabulary\n",
      "Adding <|es|> to the vocabulary\n",
      "Adding <|ru|> to the vocabulary\n",
      "Adding <|ko|> to the vocabulary\n",
      "Adding <|fr|> to the vocabulary\n",
      "Adding <|ja|> to the vocabulary\n",
      "Adding <|pt|> to the vocabulary\n",
      "Adding <|tr|> to the vocabulary\n",
      "Adding <|pl|> to the vocabulary\n",
      "Adding <|ca|> to the vocabulary\n",
      "Adding <|nl|> to the vocabulary\n",
      "Adding <|ar|> to the vocabulary\n",
      "Adding <|sv|> to the vocabulary\n",
      "Adding <|it|> to the vocabulary\n",
      "Adding <|id|> to the vocabulary\n",
      "Adding <|hi|> to the vocabulary\n",
      "Adding <|fi|> to the vocabulary\n",
      "Adding <|vi|> to the vocabulary\n",
      "Adding <|iw|> to the vocabulary\n",
      "Adding <|uk|> to the vocabulary\n",
      "Adding <|el|> to the vocabulary\n",
      "Adding <|ms|> to the vocabulary\n",
      "Adding <|cs|> to the vocabulary\n",
      "Adding <|ro|> to the vocabulary\n",
      "Adding <|da|> to the vocabulary\n",
      "Adding <|hu|> to the vocabulary\n",
      "Adding <|ta|> to the vocabulary\n",
      "Adding <|no|> to the vocabulary\n",
      "Adding <|th|> to the vocabulary\n",
      "Adding <|ur|> to the vocabulary\n",
      "Adding <|hr|> to the vocabulary\n",
      "Adding <|bg|> to the vocabulary\n",
      "Adding <|lt|> to the vocabulary\n",
      "Adding <|la|> to the vocabulary\n",
      "Adding <|mi|> to the vocabulary\n",
      "Adding <|ml|> to the vocabulary\n",
      "Adding <|cy|> to the vocabulary\n",
      "Adding <|sk|> to the vocabulary\n",
      "Adding <|te|> to the vocabulary\n",
      "Adding <|fa|> to the vocabulary\n",
      "Adding <|lv|> to the vocabulary\n",
      "Adding <|bn|> to the vocabulary\n",
      "Adding <|sr|> to the vocabulary\n",
      "Adding <|az|> to the vocabulary\n",
      "Adding <|sl|> to the vocabulary\n",
      "Adding <|kn|> to the vocabulary\n",
      "Adding <|et|> to the vocabulary\n",
      "Adding <|mk|> to the vocabulary\n",
      "Adding <|br|> to the vocabulary\n",
      "Adding <|eu|> to the vocabulary\n",
      "Adding <|is|> to the vocabulary\n",
      "Adding <|hy|> to the vocabulary\n",
      "Adding <|ne|> to the vocabulary\n",
      "Adding <|mn|> to the vocabulary\n",
      "Adding <|bs|> to the vocabulary\n",
      "Adding <|kk|> to the vocabulary\n",
      "Adding <|sq|> to the vocabulary\n",
      "Adding <|sw|> to the vocabulary\n",
      "Adding <|gl|> to the vocabulary\n",
      "Adding <|mr|> to the vocabulary\n",
      "Adding <|pa|> to the vocabulary\n",
      "Adding <|si|> to the vocabulary\n",
      "Adding <|km|> to the vocabulary\n",
      "Adding <|sn|> to the vocabulary\n",
      "Adding <|yo|> to the vocabulary\n",
      "Adding <|so|> to the vocabulary\n",
      "Adding <|af|> to the vocabulary\n",
      "Adding <|oc|> to the vocabulary\n",
      "Adding <|ka|> to the vocabulary\n",
      "Adding <|be|> to the vocabulary\n",
      "Adding <|tg|> to the vocabulary\n",
      "Adding <|sd|> to the vocabulary\n",
      "Adding <|gu|> to the vocabulary\n",
      "Adding <|am|> to the vocabulary\n",
      "Adding <|yi|> to the vocabulary\n",
      "Adding <|lo|> to the vocabulary\n",
      "Adding <|uz|> to the vocabulary\n",
      "Adding <|fo|> to the vocabulary\n",
      "Adding <|ht|> to the vocabulary\n",
      "Adding <|ps|> to the vocabulary\n",
      "Adding <|tk|> to the vocabulary\n",
      "Adding <|nn|> to the vocabulary\n",
      "Adding <|mt|> to the vocabulary\n",
      "Adding <|sa|> to the vocabulary\n",
      "Adding <|lb|> to the vocabulary\n",
      "Adding <|my|> to the vocabulary\n",
      "Adding <|bo|> to the vocabulary\n",
      "Adding <|tl|> to the vocabulary\n",
      "Adding <|mg|> to the vocabulary\n",
      "Adding <|as|> to the vocabulary\n",
      "Adding <|tt|> to the vocabulary\n",
      "Adding <|haw|> to the vocabulary\n",
      "Adding <|ln|> to the vocabulary\n",
      "Adding <|ha|> to the vocabulary\n",
      "Adding <|ba|> to the vocabulary\n",
      "Adding <|jw|> to the vocabulary\n",
      "Adding <|su|> to the vocabulary\n",
      "Adding <|translate|> to the vocabulary\n",
      "Adding <|transcribe|> to the vocabulary\n",
      "Adding <|startoflm|> to the vocabulary\n",
      "Adding <|startofprev|> to the vocabulary\n",
      "Adding <|nocaptions|> to the vocabulary\n",
      "Adding <|notimestamps|> to the vocabulary\n",
      "The following columns in the test set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: norm, labels_length, input_length, feature_length. If norm, labels_length, input_length, feature_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 472\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/15 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 10-12 Ṭhbhag.', ' hatharena warisankhe enthe ekku antesain pita sqeed lojerin antakriyelant churranan ke andri ki sadhe padalidhu.', ' Khevalam konnisvalpa beidaalutappa, kaalpanika brundaalukoda, sampradhaya brundaaluvanti naipunyam kaligumantai.', ' Aerosmith wari paryatana lo miglinal sangi te kacheri leno dhadhu chaisindi.', ' i Ṛṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭṭ�'] ['రెండవ సెట్\\u200cలో Del Potroకు ఆధిక్యం లభించినా కూడా, ఈ సెట్\\u200cలో కూడా 6-6కి చేరుకున్న తర్వాత టై బ్రేక్ అనివార్యం అయ్యింది.', \"హాజరైన వారి సంఖ్య ఎంత ఎక్కువంటే, St. Peter's స్క్వేర్\\u200cలో జరిగిన అంత్యక్రియలను చూడడానికి అందరికీ సాధ్యపడలేదు.\", 'కేవలం కొన్ని స్వల్ప బేధాలు తప్ప, కాల్పనిక బృందాలు కూడా సాంప్రదాయ బృందాల వంటి నైపుణ్యం కలిగి ఉంటాయి.', 'ఏరోస్మిత్ వారి పర్యటనలో మిగిలిన సంగీత కచేరీలను రద్దు చేసింది.', 'ఈ ప్రమాదం ఎత్తుగా ఉన్న పర్వత భూభాగంలో జరిగిందని, ఇంకా శత్రువులు రాజేసిన అగ్ని ఫలితంగా జరిగిందని నమ్ముతారు.']\n",
      "***** test2 metrics *****\n",
      "  test_loss               =     1.9748\n",
      "  test_runtime            = 0:05:36.74\n",
      "  test_samples_per_second =      1.402\n",
      "  test_steps_per_second   =      0.045\n",
      "  test_wer                =   170.1404\n",
      "  test_wer_norm           =    111.709\n",
      "Evaluation for cmn_hans_cn starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/vocab.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file merges.txt from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/merges.txt\n",
      "loading file normalizer.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/normalizer.json\n",
      "loading file added_tokens.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/tokenizer_config.json\n",
      "Adding <|endoftext|> to the vocabulary\n",
      "Adding <|startoftranscript|> to the vocabulary\n",
      "Adding <|en|> to the vocabulary\n",
      "Adding <|zh|> to the vocabulary\n",
      "Adding <|de|> to the vocabulary\n",
      "Adding <|es|> to the vocabulary\n",
      "Adding <|ru|> to the vocabulary\n",
      "Adding <|ko|> to the vocabulary\n",
      "Adding <|fr|> to the vocabulary\n",
      "Adding <|ja|> to the vocabulary\n",
      "Adding <|pt|> to the vocabulary\n",
      "Adding <|tr|> to the vocabulary\n",
      "Adding <|pl|> to the vocabulary\n",
      "Adding <|ca|> to the vocabulary\n",
      "Adding <|nl|> to the vocabulary\n",
      "Adding <|ar|> to the vocabulary\n",
      "Adding <|sv|> to the vocabulary\n",
      "Adding <|it|> to the vocabulary\n",
      "Adding <|id|> to the vocabulary\n",
      "Adding <|hi|> to the vocabulary\n",
      "Adding <|fi|> to the vocabulary\n",
      "Adding <|vi|> to the vocabulary\n",
      "Adding <|iw|> to the vocabulary\n",
      "Adding <|uk|> to the vocabulary\n",
      "Adding <|el|> to the vocabulary\n",
      "Adding <|ms|> to the vocabulary\n",
      "Adding <|cs|> to the vocabulary\n",
      "Adding <|ro|> to the vocabulary\n",
      "Adding <|da|> to the vocabulary\n",
      "Adding <|hu|> to the vocabulary\n",
      "Adding <|ta|> to the vocabulary\n",
      "Adding <|no|> to the vocabulary\n",
      "Adding <|th|> to the vocabulary\n",
      "Adding <|ur|> to the vocabulary\n",
      "Adding <|hr|> to the vocabulary\n",
      "Adding <|bg|> to the vocabulary\n",
      "Adding <|lt|> to the vocabulary\n",
      "Adding <|la|> to the vocabulary\n",
      "Adding <|mi|> to the vocabulary\n",
      "Adding <|ml|> to the vocabulary\n",
      "Adding <|cy|> to the vocabulary\n",
      "Adding <|sk|> to the vocabulary\n",
      "Adding <|te|> to the vocabulary\n",
      "Adding <|fa|> to the vocabulary\n",
      "Adding <|lv|> to the vocabulary\n",
      "Adding <|bn|> to the vocabulary\n",
      "Adding <|sr|> to the vocabulary\n",
      "Adding <|az|> to the vocabulary\n",
      "Adding <|sl|> to the vocabulary\n",
      "Adding <|kn|> to the vocabulary\n",
      "Adding <|et|> to the vocabulary\n",
      "Adding <|mk|> to the vocabulary\n",
      "Adding <|br|> to the vocabulary\n",
      "Adding <|eu|> to the vocabulary\n",
      "Adding <|is|> to the vocabulary\n",
      "Adding <|hy|> to the vocabulary\n",
      "Adding <|ne|> to the vocabulary\n",
      "Adding <|mn|> to the vocabulary\n",
      "Adding <|bs|> to the vocabulary\n",
      "Adding <|kk|> to the vocabulary\n",
      "Adding <|sq|> to the vocabulary\n",
      "Adding <|sw|> to the vocabulary\n",
      "Adding <|gl|> to the vocabulary\n",
      "Adding <|mr|> to the vocabulary\n",
      "Adding <|pa|> to the vocabulary\n",
      "Adding <|si|> to the vocabulary\n",
      "Adding <|km|> to the vocabulary\n",
      "Adding <|sn|> to the vocabulary\n",
      "Adding <|yo|> to the vocabulary\n",
      "Adding <|so|> to the vocabulary\n",
      "Adding <|af|> to the vocabulary\n",
      "Adding <|oc|> to the vocabulary\n",
      "Adding <|ka|> to the vocabulary\n",
      "Adding <|be|> to the vocabulary\n",
      "Adding <|tg|> to the vocabulary\n",
      "Adding <|sd|> to the vocabulary\n",
      "Adding <|gu|> to the vocabulary\n",
      "Adding <|am|> to the vocabulary\n",
      "Adding <|yi|> to the vocabulary\n",
      "Adding <|lo|> to the vocabulary\n",
      "Adding <|uz|> to the vocabulary\n",
      "Adding <|fo|> to the vocabulary\n",
      "Adding <|ht|> to the vocabulary\n",
      "Adding <|ps|> to the vocabulary\n",
      "Adding <|tk|> to the vocabulary\n",
      "Adding <|nn|> to the vocabulary\n",
      "Adding <|mt|> to the vocabulary\n",
      "Adding <|sa|> to the vocabulary\n",
      "Adding <|lb|> to the vocabulary\n",
      "Adding <|my|> to the vocabulary\n",
      "Adding <|bo|> to the vocabulary\n",
      "Adding <|tl|> to the vocabulary\n",
      "Adding <|mg|> to the vocabulary\n",
      "Adding <|as|> to the vocabulary\n",
      "Adding <|tt|> to the vocabulary\n",
      "Adding <|haw|> to the vocabulary\n",
      "Adding <|ln|> to the vocabulary\n",
      "Adding <|ha|> to the vocabulary\n",
      "Adding <|ba|> to the vocabulary\n",
      "Adding <|jw|> to the vocabulary\n",
      "Adding <|su|> to the vocabulary\n",
      "Adding <|translate|> to the vocabulary\n",
      "Adding <|transcribe|> to the vocabulary\n",
      "Adding <|startoflm|> to the vocabulary\n",
      "Adding <|startofprev|> to the vocabulary\n",
      "Adding <|nocaptions|> to the vocabulary\n",
      "Adding <|notimestamps|> to the vocabulary\n",
      "***** Running Prediction *****\n",
      "  Num examples = 945\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/30 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1940年8月15日蒙軍公路法國南部這次進攻被稱為龍旗冰晴洞', '該全島位於南極半島一杯120公里處最大的島嶼是橋至國王島這裡是凡青春的定居點', '卷心菜之的颜色将根据化学物质的酸检性发生变化', '海地正與民主研究所引用的獨立研究表明是尼波爾的聯合國維和部隊在不知情的情況下將這種疾病帶到了海地', '老虎是全能運動員,他會犯罰,雖然犯罰能力不是很強,游泳,跳遠,拉力能達到人類大力勢的五倍'] ['1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。', '该群岛位于南极半岛以北 120 公里处。最大的岛屿是乔治国王岛，这里是“繁星村（Villa Las Estrellas）”的定居点。', '卷心菜汁的颜色将根据化学物质的酸碱性发生变化。', '海地正义与民主研究所 (Haitian Institute for Justice and Democracy) 引用的独立研究表明，是尼泊尔的联合国维和部队在不知情的情况下将这种疾病带到了海地。', '老虎是全能运动员，它会攀爬（虽然攀爬能力不是很强）、游泳、跳远，拉力能达到人类大力士的五倍。']\n",
      "***** test2 metrics *****\n",
      "  test_loss               =     1.7503\n",
      "  test_runtime            = 0:05:04.18\n",
      "  test_samples_per_second =      3.107\n",
      "  test_steps_per_second   =      0.099\n",
      "  test_wer                =   103.5536\n",
      "  test_wer_norm           =    35.1913\n",
      "CPU times: user 17min 10s, sys: 3min 30s, total: 20min 41s\n",
      "Wall time: 18min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lang in langs:\n",
    "    print(f\"Evaluation for {lang} starts...\")\n",
    "\n",
    "    # load pretrained model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "\n",
    "    # model.config.forced_decoder_ids = None\n",
    "    # model.config.suppress_tokens = []\n",
    "    # model.config.use_cache = False\n",
    "\n",
    "    # uncomment for evaluate\n",
    "    model.config.forced_decoder_ids = processors[lang].get_decoder_prompt_ids(language=codes[lang], task=\"transcribe\")\n",
    "\n",
    "    # redefine output path\n",
    "    training_args.output_dir = f\"/home/sivan/whisper_base_fl_{lang}\"\n",
    "\n",
    "    # specify metric for each language\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=languages[lang], task=\"transcribe\")\n",
    "    compute_metrics = utils.metrics(codes[lang], tokenizer)\n",
    "\n",
    "    # set trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        # train_dataset=fleurs[lang][\"train\"],\n",
    "        # eval_dataset=fleurs[lang][\"validation\"],\n",
    "        data_collator=collators[lang],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processors[lang].feature_extractor,\n",
    "    )\n",
    "\n",
    "    #     trainer.train()\n",
    "\n",
    "    predict_results = trainer.predict(preprocess[lang]['test'], metric_key_prefix=\"test\")\n",
    "    metrics = predict_results.metrics\n",
    "    trainer.log_metrics(\"test2\", metrics)\n",
    "    trainer.save_metrics(\"test2\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for ko_kr starts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40230116c624d1584824680665ef1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from pretrain Whisper base:\n",
      "WER_NORM: 0.3330376153300213\n",
      "WER: 0.3567375886524823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434c6a3556924189ae9099c48fa9fbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from finetune Whisper base:\n",
      "WER_NORM: 0.2940028388928318\n",
      "WER: 0.3129432624113475\n",
      "Evaluation for he_il starts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf0a2621a3e43dabe6b2f4a8ea45205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/792 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from pretrain Whisper base:\n",
      "WER_NORM: 0.7137796147317145\n",
      "WER: 0.7522901891252955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff4fb3c2bb54736bf7653e6b9652c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/792 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from finetune Whisper base:\n",
      "WER_NORM: 0.5929588899549783\n",
      "WER: 0.619533096926714\n",
      "Evaluation for te_in starts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525ce3a7793b4f5ba8b2796b9698a7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from pretrain Whisper base:\n",
      "WER_NORM: 2.5908219353189312\n",
      "WER: 2.474859622256253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfed83872b24a12a3ab9d8cd6bdc0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from finetune Whisper base:\n",
      "WER_NORM: 2.9284162086156207\n",
      "WER: 0.6856814701378254\n",
      "Evaluation for cmn_hans_cn starts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bf496d7e5f48cab9b92c392b4b8504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/945 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# compare with vanilla eval method\n",
    "from jiwer import wer\n",
    "\n",
    "for lang in langs:\n",
    "    print(f\"Evaluation for {lang} starts...\")\n",
    "\n",
    "    # load processor\n",
    "    # processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "    # load pretrained model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "    model.config.forced_decoder_ids = processors[lang].get_decoder_prompt_ids(language=codes[lang], task=\"transcribe\")\n",
    "    result = tests[lang].map(utils.map_to_pred, \n",
    "                             fn_kwargs={\"model\": model, \"processor\": processors[lang], \"lang\": codes[lang]},\n",
    "                             batched=True, batch_size=1)\n",
    "    print(\"Results from pretrain Whisper base:\")\n",
    "    print(\"WER_NORM:\", wer(result[\"transcription\"], result[\"prediction\"]))\n",
    "    print(\"WER:\", wer(result[\"raw_transcription\"], result[\"raw_prediction\"]))\n",
    "\n",
    "    # load finetuned model\n",
    "    model_tune = WhisperForConditionalGeneration.from_pretrained(f\"/home/sivan/whisper_base_fl_{lang}/checkpoint-{best[lang]}\").to(\"cuda\")\n",
    "\n",
    "    model_tune.config.forced_decoder_ids = processors[lang].get_decoder_prompt_ids(language=codes[lang], task=\"transcribe\")\n",
    "    result_tune = tests[lang].map(utils.map_to_pred, \n",
    "                                  fn_kwargs={\"model\": model_tune, \"processor\": processors[lang], \"lang\": codes[lang]},\n",
    "                                  batched=True, batch_size=1)\n",
    "    print(\"Results from finetune Whisper base:\")\n",
    "    print(\"WER_NORM:\", wer(result_tune[\"transcription\"], result_tune[\"prediction\"]))\n",
    "    print(\"WER:\", wer(result_tune[\"raw_transcription\"], result_tune[\"raw_prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from /home/sivan/whisper_base_fl_te_in/checkpoint-4000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: norm, labels_length, feature_length, input_length. If norm, labels_length, feature_length, input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2302\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 27\n",
      "  Continuing training from global step 4000\n",
      "  Will skip the first 27 epochs then the first 112 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e5af570cae450cab22e6addfcfe6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4001' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 00:01, Epoch 27/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/sivan/whisper_base_fl_te_in/checkpoint-1000 (score: 71.45242070116862).\n",
      "The following columns in the test set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: norm, labels_length, feature_length, input_length. If norm, labels_length, feature_length, input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 472\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['అనెండవ సెట్ లో డెల్ పోట్రాకు ఆధిక్యం లభించిన కూడా ఈ సెట్ లో కూడా ఆరు ఆర్నించి ఆరికీ చేరుకున్న తర్వాత డైబ్రే గణివారియం అ�', 'హాథరైనవారిసంకే ఎంత ఎక్కువంటే సెన్ పీటస్ క్వేడ్ లు జరీ నంత క్రీలన్ చూగడానికి అందరికి సాధ్యపడలేదు.', 'కేవలం కొన్ని స్వల్ప భేదాలు తప్ప కాల్పనిక బృందాలు కూడా సంప్రదాయ బృందాలు వంటి నైకున్యం కలిగి ఉంటాయి.', 'ఏరోస్మిద్వారి పర్యటనలో మిగిలిన సంగీత కచేరీలను రద్దుచేసింది.', 'ఈ ప్రమాదం ఎత్తుగా ఉన్న పర్వత భూభాగంలో జరిగిన్నని ఇంకా శెత్రులు రాజేసిన అగ్ని ఫలితంగా జరిగిన్నని నమ్ముతారు.'] ['రెండవ సెట్\\u200cలో Del Potroకు ఆధిక్యం లభించినా కూడా, ఈ సెట్\\u200cలో కూడా 6-6కి చేరుకున్న తర్వాత టై బ్రేక్ అనివార్యం అయ్యింది.', \"హాజరైన వారి సంఖ్య ఎంత ఎక్కువంటే, St. Peter's స్క్వేర్\\u200cలో జరిగిన అంత్యక్రియలను చూడడానికి అందరికీ సాధ్యపడలేదు.\", 'కేవలం కొన్ని స్వల్ప బేధాలు తప్ప, కాల్పనిక బృందాలు కూడా సాంప్రదాయ బృందాల వంటి నైపుణ్యం కలిగి ఉంటాయి.', 'ఏరోస్మిత్ వారి పర్యటనలో మిగిలిన సంగీత కచేరీలను రద్దు చేసింది.', 'ఈ ప్రమాదం ఎత్తుగా ఉన్న పర్వత భూభాగంలో జరిగిందని, ఇంకా శత్రువులు రాజేసిన అగ్ని ఫలితంగా జరిగిందని నమ్ముతారు.']\n",
      "***** test3 metrics *****\n",
      "  test_loss               =     0.2943\n",
      "  test_runtime            = 0:05:48.17\n",
      "  test_samples_per_second =      1.356\n",
      "  test_steps_per_second   =      0.043\n",
      "  test_wer                =     71.493\n",
      "  test_wer_norm           =     40.339\n"
     ]
    }
   ],
   "source": [
    "fleurs_te = preprocess[\"te_in\"]\n",
    "collator = collators[\"te_in\"]\n",
    "processor = processors[\"te_in\"]\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/sivan/whisper_base_fl_te_in\",\n",
    "    per_device_train_batch_size=16,  # originally 16\n",
    "    gradient_accumulation_steps=1,  # originally 1, increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # original True\n",
    "    group_by_length=False,  # set true if length is specified in dataset\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,  # set true to push trained model to HF\n",
    "    disable_tqdm=False,  # set false to see progress bar\n",
    ")\n",
    "\n",
    "# load pretrained model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False\n",
    "\n",
    "# specify metric for each language\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Telugu\", task=\"transcribe\")\n",
    "compute_metrics = utils.metrics(\"te\", tokenizer)\n",
    "\n",
    "\n",
    "# set trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=fleurs_te[\"train\"],\n",
    "    eval_dataset=fleurs_te[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(f\"Training starts...\")\n",
    "trainer.train(resume_from_checkpoint = True) # comment to eval original whisper base\n",
    "\n",
    "predict_results = trainer.predict(fleurs_te[\"test\"], metric_key_prefix=\"test\")\n",
    "metrics = predict_results.metrics\n",
    "trainer.log_metrics(\"test3\", metrics)\n",
    "trainer.save_metrics(\"test3\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}