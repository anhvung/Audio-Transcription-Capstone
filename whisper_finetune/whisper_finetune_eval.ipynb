{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate finetuned Whisper on Fleurs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reproduce reported results to check validity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check text tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# yue_hant_hk for Cantonese and cmn_hans_cn for Chinese\n",
    "dataset = load_dataset(\"google/fleurs\", \"cmn_hans_cn\", split='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "    num_rows: 945\n",
      "})\n",
      "{'audio': {'path': '11338778690242874435.wav', 'array': array([ 4.17232513e-07, -5.36441803e-07,  5.96046448e-07, ...,\n",
      "       -1.31678581e-03, -1.24770403e-03, -1.29789114e-03]), 'sampling_rate': 16000}, 'transcription': '1940 年 8 月 15 日 盟 军 攻 入 法 国 南 部 这 次 进 攻 被 称 为 龙 骑 兵 行 动', 'raw_transcription': '1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "# print(dataset[0])\n",
    "dataset = dataset.remove_columns(['id', 'num_samples', 'path', 'gender', 'lang_id','language','lang_group_id'])\n",
    "print(dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Chinese\", task=\"transcribe\")\n",
    "# processor\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Chinese\", task=\"transcribe\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。\n",
      "Decoded w/ special:    <|startoftranscript|><|zh|><|transcribe|><|notimestamps|>1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。<|endoftext|>\n",
      "Decoded w/out special: 1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = dataset[0][\"raw_transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check base model on Chinese (simplified and cantonese)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "def map_to_pred(batch):\n",
    "    sampling_rate = batch.features[\"audio\"].sampling_rate\n",
    "    input_features = processor(batch[\"audio\"][0][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    generated_ids = model.generate(inputs=input_features.to(\"cuda\"))\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True,\n",
    "                                           # normalize=True\n",
    "                                           )\n",
    "    # batch[\"logits\"] = generated_ids.cpu().detach().numpy()\n",
    "    batch['prediction'] = [custom_normalizer(transcription[0], \"zh\")]\n",
    "    # batch['prediction'] = transcription[0]\n",
    "    # batch['ground_truth'] = processor.tokenizer._normalize(batch['transcription'][0])\n",
    "    # batch['ground_truth'] = [normalizer(batch['transcription'][0])]\n",
    "    # batch['ground_truth'] = batch['transcription']\n",
    "    # print(batch)\n",
    "    return batch\n",
    "\n",
    "def custom_normalizer(text, lang):\n",
    "    \"\"\"\n",
    "    normalizing procedures based on appendix C, Whisper OpenAI paper\n",
    "    language tokens based on https://github.com/openai/whisper/blob/main/whisper/tokenizer.py\n",
    "    \"\"\"\n",
    "    if lang == 'en':\n",
    "        return normalizer(text)\n",
    "    else:\n",
    "        text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text) # removes [] and () as well as content in-between -- will not work for non-standard brackets, eg: <> or （）, etc\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        ch_text = []\n",
    "        for ch in text:\n",
    "            if unicodedata.category(ch)[0] not in ('M', 'P', 'S'):\n",
    "                ch_text.append(ch)\n",
    "            else:\n",
    "                ch_text.append(' ')\n",
    "        text = ''.join(ch_text)\n",
    "        text = text.lower()\n",
    "    # set up for character error rate for languages w/o spaces between words\n",
    "    if lang in ('zh', 'ja', 'th', 'lo', 'my'):\n",
    "        text = ' '.join(text)\n",
    "        text = re.sub('(?<=\\d) (?=\\d)', '', text)\n",
    "    return re.sub(' +', ' ', text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/945 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8b079a7622e45f8a42007ba2fd7114a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "# using \"zh\" in model => traditional chinese whereas Fleurs uses simplified Chinese for cmn_hans_cn, and yue_hant_hk\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"zh\", task = \"transcribe\")\n",
    "result = dataset.map(map_to_pred, batched=True, batch_size = 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.38239503090460675\n"
     ]
    }
   ],
   "source": [
    "print(\"WER:\", wer(result[\"ground_truth\"], result[\"prediction\"])) # (truth, pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': None, 'array': array([ 0.00000000e+00, -3.05175781e-05,  0.00000000e+00, ...,\n",
      "       -1.34277344e-03, -1.25122070e-03, -1.31225586e-03]), 'sampling_rate': 16000}, 'transcription': '1940 年 8 月 15 日 盟 军 攻 入 法 国 南 部 这 次 进 攻 被 称 为 龙 骑 兵 行 动', 'raw_transcription': '1940 年 8 月 15 日，盟军攻入法国南部，这次进攻被称为“龙骑兵行动”。', 'prediction': '1940 年 8 月 15 日 蒙 軍 公 路 法 國 南 部 這 次 進 攻 被 稱 為 龍 旗 冰 晴 洞', 'ground_truth': '1940 年 8 月 15 日 盟 军 攻 入 法 国 南 部 这 次 进 攻 被 称 为 龙 骑 兵 行 动'}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/yue_hant_hk/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# yue_hant_hk for Cantonese and cmn_hans_cn for Chinese\n",
    "dataset = load_dataset(\"google/fleurs\", \"yue_hant_hk\", split='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "    num_rows: 819\n",
      "})\n",
      "{'audio': {'path': '4705644860951940224.wav', 'array': array([ 0.        ,  0.        ,  0.        , ...,  0.00025684,\n",
      "       -0.00013679, -0.00051916]), 'sampling_rate': 16000}, 'transcription': '仍 有 許 多 當 時 在 此 的 男 女 存 活 了 下 來 還 有 更 多 人 的 摯 愛 在 此 被 殺 害 或 勞 動 至 死 不 管 是 不 是 猶 太'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "# print(dataset[0])\n",
    "dataset = dataset.remove_columns(['id', 'num_samples', 'path', 'gender', 'lang_id','language','lang_group_id', 'raw_transcription'])\n",
    "print(dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/819 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "387e40a74e2146888291eae63678aa33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 57s, sys: 14.3 s, total: 7min 11s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "# using \"zh\" in model => traditional chinese whereas Fleurs uses simplified Chinese for cmn_hans_cn, and yue_hant_hk\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"zh\", task = \"transcribe\")\n",
    "result = dataset.map(map_to_pred, batched=True, remove_columns=['audio'], batch_size = 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.4356259136449531\n"
     ]
    }
   ],
   "source": [
    "print(\"WER:\", wer(result[\"transcription\"], result[\"prediction\"])) # (truth, pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': '仍 有 許 多 當 時 在 此 的 男 女 存 活 了 下 來 還 有 更 多 人 的 摯 愛 在 此 被 殺 害 或 勞 動 至 死 不 管 是 不 是 猶 太', 'prediction': '仍 有 許 多 當 時 在 此 的 男 女 徐 樂 了 下 來 還 有 更 多 人 的 自 愛 在 此 被 殺 害 惑 努 動 之 死 不 過 是 不 事 由 太 '}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}