{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Finetuning on Fleurs KO, TE, HE\n",
    "\n",
    "Check whisper_tinetune_demo notebook for finetuning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths for input/output\n",
    "root = os.getcwd()\n",
    "datasets_path = os.path.join(root, 'datasets')\n",
    "predictions_path = os.path.join(root, 'predictions')\n",
    "#sivan_path = '/home/sivan/datasets/'\n",
    "# create folders if they do not already exist\n",
    "if not os.path.exists(datasets_path): os.makedirs(datasets_path)\n",
    "if not os.path.exists(predictions_path): os.makedirs(predictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "langs = [\"te_in\", \"ko_kr\", \"he_il\"]\n",
    "codes = {\"te_in\": 'Telugu', \"ko_kr\": 'Korean', \"he_il\": 'Hebrew'}\n",
    "fleurs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0912ba9d33460db61ec6eda8a5959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-c818ff5160121eec.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-8d0f61e940315fc0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-99ebe6087268c447.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-dbbc8c71e3041f1c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-9d79f83762d90db3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-3ae4014f8d349b30.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-ede513a80a3f2613.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-def57ab2c5d9b2ea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-a812162401ab3938.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-8acccfdb5de4bea0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-a52c8602b1413c62.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-682da7e22d076104.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/ko_kr/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da680dbcfe104586a62826153746f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad402d4b2a374466bf0778fddda7370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/577 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335a863c810345c89fa75613b4e4ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/577 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dd9eea65584c34868bfb5e9e6de7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/577 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b855fb0277c547498dd1dd0c96b0eb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/576 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29eaefedb0d45fc88c1be60421180b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/57 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731e5f0f25684a0486b015839a3bd097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/57 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beeff65b93c4275866d4a9bda2a84fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/56 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9f4f297e284880a0310d8b30e2d998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/56 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39e40ac93654c95be152a492231ce2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/96 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3600b78d1e94678a10643044d30f17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/96 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b599b0c65e4d0a8ca503eb942d62cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/95 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f00cc3502ce4929ae95cb8408328a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/95 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/he_il/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df92c9e0bd644b28f3da74e8c57a52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5cbdc961204e02b092c7723f5a3526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/811 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b6fdf93dd64877b4720e6a544f4b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/811 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5002632a514376882b2452d3472c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/810 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e07349588764d2d8328d4e75db9137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/810 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e86ec06b6e4d13b6b13653075cf348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedbac7f83084107ba47693513813bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a9ca12714148beb749e07fd10f90d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2201179b33da4b0ebd5ca9f5877757d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b0a1237498421b9025e15251a93c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dda9c8e4c742a29f3dbd05eca108b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b66ba0f94a04539ba94ed582ebd7df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585da4d6d77f4c44b8cebfe6e45b0716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "CPU times: user 1min 16s, sys: 13.4 s, total: 1min 30s\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "% % time  # only counts preprocessing and not loading, given cached datasets -- even for preprocessing, should add ~5 minutes to total time for loading/processing of telugu\n",
    "\n",
    "for lang in langs:\n",
    "    print(\"Loading and preprocessing dataset...\")\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=codes[lang], task=\"transcribe\")\n",
    "    fleurs[lang] = DatasetDict(load_dataset('google/fleurs', lang))\n",
    "    fleurs[lang] = fleurs[lang].map(utils.prepare_dataset,\n",
    "                                    fn_kwargs={\"feature_extractor\": feature_extractor, \"tokenizer\": tokenizer},\n",
    "                                    remove_columns=fleurs[lang].column_names[\"train\"], num_proc=4)\n",
    "    print(\"Saving dataset...\")\n",
    "    fleurs[lang].save_to_disk(os.path.join(datasets_path, 'fleurs_' + lang + '_features'))\n",
    "    #fleurs[lang].save_to_disk(os.path.join(sivan_path, 'fl_' + lang + '_features')) <--- no permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'te_in': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "}), 'ko_kr': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2307\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 226\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 382\n",
      "    })\n",
      "}), 'he_il': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3242\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 792\n",
      "    })\n",
      "})}\n",
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 472\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fleurs)\n",
    "print(fleurs['te_in']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./datasets/fleurs_te_in_features/dataset_dict.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/test/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/test/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/test/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./datasets/fleurs_te_in_features/validation/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/validation/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/validation/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_te_in_features/train/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/train/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_te_in_features/train/dataset_info.json [Content-Type=application/json]...\n",
      "- [10/10 files][  2.8 GiB/  2.8 GiB] 100% Done  40.0 MiB/s ETA 00:00:00         \n",
      "Operation completed over 10 objects/2.8 GiB.                                     \n",
      "Copying file://./datasets/fleurs_ko_kr_features/dataset_dict.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/test/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/test/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/validation/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/test/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./datasets/fleurs_ko_kr_features/validation/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/validation/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/train/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/train/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/train/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "| [10/10 files][  2.6 GiB/  2.6 GiB] 100% Done  40.8 MiB/s ETA 00:00:00         \n",
      "Operation completed over 10 objects/2.6 GiB.                                     \n",
      "Copying file://./datasets/fleurs_he_il_features/dataset_dict.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/test/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/test/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/test/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./datasets/fleurs_he_il_features/validation/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/validation/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_he_il_features/train/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_he_il_features/train/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/validation/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/train/dataset_info.json [Content-Type=application/json]...\n",
      "/ [10/10 files][  3.9 GiB/  3.9 GiB] 100% Done  39.5 MiB/s ETA 00:00:00         \n",
      "Operation completed over 10 objects/3.9 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# removed -n to allow overwrite\n",
    "\n",
    "!gsutil -m cp -r./ datasets / fleurs_te_in_features gs: // capstone_datasets / fleurs / preprocess / fl_te_features\n",
    "!gsutil -m cp -r./ datasets / fleurs_ko_kr_features gs: // capstone_datasets / fleurs / preprocess / fl_ko_features\n",
    "!gsutil -m cp -r./ datasets / fleurs_he_il_features gs: // capstone_datasets / fleurs / preprocess / fl_iw_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Preprocessed Data and Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from datasets import load_from_disk\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "langs = [\"te_in\", \"ko_kr\", \"he_il\"]\n",
    "codes = {\"te_in\":'te', \"ko_kr\":'ko', \"he_il\":'iw'}\n",
    "# codes = {\"te_in\": 'Telugu', \"ko_kr\": 'Korean', \"he_il\": 'Hebrew'}\n",
    "fleurs = {}\n",
    "models = {}\n",
    "collators = {}\n",
    "processors={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2307\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 226\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 382\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3242\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 792\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    # load dataset from disk\n",
    "    fleurs[lang] = load_from_disk(f\"/home/alexandriaguo/datasets/fleurs_{lang}_features\")\n",
    "    print(fleurs[lang])\n",
    "\n",
    "    # load whisper processor\n",
    "    processors[lang] = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=lang, task=\"transcribe\")\n",
    "\n",
    "    # initialize data collator\n",
    "    collators[lang] = utils.DataCollatorSpeechSeq2SeqWithPadding(processor=processors[lang])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# universal training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/sivan\",\n",
    "    per_device_train_batch_size=16,  # originally 16\n",
    "    gradient_accumulation_steps=1,  # originally 1, increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # original True\n",
    "    group_by_length=False,  # set true if length is specified in dataset\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,  # set true to push trained model to HF\n",
    "    disable_tqdm=False,  # set false to see progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for te_in starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2302\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/4000 00:22 < 6:08:06, 0.18 it/s, Epoch 0.03/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "for lang in langs:\n",
    "    print(f\"Training for {lang} starts...\")\n",
    "\n",
    "    # load pretrained model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # redefine output path\n",
    "    training_args.output_dir = f\"/home/sivan/whisper_base_fl_{lang}\"\n",
    "\n",
    "    # specify metric for each language\n",
    "    compute_metrics = utils.metrics(codes[lang])\n",
    "\n",
    "\n",
    "    # set trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=fleurs[lang][\"train\"],\n",
    "        eval_dataset=fleurs[lang][\"validation\"],\n",
    "        data_collator=collators[lang],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processors[lang].feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    predict_results = trainer.predict(fleurs[lang][\"test\"], metric_key_prefix=\"test\")\n",
    "    metrics = predict_results.metrics\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "    trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with best finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
