{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Finetuning on Fleurs KO, TE, HE\n",
    "\n",
    "Check whisper_tinetune_demo notebook for finetuning pipeline\n",
    "\n",
    "Details of debugging Telugu preprocessing can be found in [this thread](https://discuss.huggingface.co/t/trainer-runtimeerror-the-size-of-tensor-a-462-must-match-the-size-of-tensor-b-448-at-non-singleton-dimension-1/26010/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 18:46:22.423662: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 18:46:22.766193: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 18:46:23.548829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-13 18:46:23.548953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-13 18:46:23.548964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths for input/output\n",
    "root = os.getcwd()\n",
    "datasets_path = os.path.join(root, 'datasets')\n",
    "predictions_path = os.path.join(root, 'predictions')\n",
    "#sivan_path = '/home/sivan/datasets/'\n",
    "# create folders if they do not already exist\n",
    "if not os.path.exists(datasets_path): os.makedirs(datasets_path)\n",
    "if not os.path.exists(predictions_path): os.makedirs(predictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "langs = [\"te_in\", \"ko_kr\", \"he_il\"]\n",
    "codes = {\"te_in\": 'Telugu', \"ko_kr\": 'Korean', \"he_il\": 'Hebrew'}\n",
    "fleurs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0912ba9d33460db61ec6eda8a5959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-c818ff5160121eec.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-8d0f61e940315fc0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-99ebe6087268c447.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-dbbc8c71e3041f1c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-9d79f83762d90db3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-3ae4014f8d349b30.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-ede513a80a3f2613.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-def57ab2c5d9b2ea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-a812162401ab3938.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-8acccfdb5de4bea0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-a52c8602b1413c62.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a/cache-682da7e22d076104.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/ko_kr/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da680dbcfe104586a62826153746f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad402d4b2a374466bf0778fddda7370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/577 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335a863c810345c89fa75613b4e4ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/577 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dd9eea65584c34868bfb5e9e6de7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/577 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b855fb0277c547498dd1dd0c96b0eb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/576 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29eaefedb0d45fc88c1be60421180b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/57 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731e5f0f25684a0486b015839a3bd097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/57 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beeff65b93c4275866d4a9bda2a84fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/56 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9f4f297e284880a0310d8b30e2d998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/56 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39e40ac93654c95be152a492231ce2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/96 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3600b78d1e94678a10643044d30f17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/96 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b599b0c65e4d0a8ca503eb942d62cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/95 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f00cc3502ce4929ae95cb8408328a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/95 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/alexandriaguo/.cache/huggingface/datasets/google___fleurs/he_il/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df92c9e0bd644b28f3da74e8c57a52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5cbdc961204e02b092c7723f5a3526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/811 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b6fdf93dd64877b4720e6a544f4b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/811 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5002632a514376882b2452d3472c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/810 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e07349588764d2d8328d4e75db9137b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/810 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e86ec06b6e4d13b6b13653075cf348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedbac7f83084107ba47693513813bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a9ca12714148beb749e07fd10f90d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2201179b33da4b0ebd5ca9f5877757d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/82 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b0a1237498421b9025e15251a93c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dda9c8e4c742a29f3dbd05eca108b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b66ba0f94a04539ba94ed582ebd7df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585da4d6d77f4c44b8cebfe6e45b0716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/198 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "CPU times: user 1min 16s, sys: 13.4 s, total: 1min 30s\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "% % time  # only counts preprocessing and not loading, given cached datasets -- even for preprocessing, should add ~5 minutes to total time for loading/processing of telugu\n",
    "\n",
    "for lang in langs:\n",
    "    print(\"Loading and preprocessing dataset...\")\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=codes[lang], task=\"transcribe\")\n",
    "    fleurs[lang] = DatasetDict(load_dataset('google/fleurs', lang))\n",
    "    fleurs[lang] = fleurs[lang].map(utils.prepare_dataset,\n",
    "                                    fn_kwargs={\"feature_extractor\": feature_extractor, \"tokenizer\": tokenizer},\n",
    "                                    remove_columns=fleurs[lang].column_names[\"train\"], num_proc=4)\n",
    "    print(\"Saving dataset...\")\n",
    "    fleurs[lang].save_to_disk(os.path.join(datasets_path, 'fleurs_' + lang + '_features'))\n",
    "    #fleurs[lang].save_to_disk(os.path.join(sivan_path, 'fl_' + lang + '_features')) <--- no permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'te_in': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "}), 'ko_kr': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2307\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 226\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 382\n",
      "    })\n",
      "}), 'he_il': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3242\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 792\n",
      "    })\n",
      "})}\n",
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 472\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fleurs)\n",
    "print(fleurs['te_in']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./datasets/fleurs_te_in_features/dataset_dict.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/test/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/test/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/test/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./datasets/fleurs_te_in_features/validation/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/validation/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/validation/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_te_in_features/train/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_te_in_features/train/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_te_in_features/train/dataset_info.json [Content-Type=application/json]...\n",
      "- [10/10 files][  2.8 GiB/  2.8 GiB] 100% Done  40.0 MiB/s ETA 00:00:00         \n",
      "Operation completed over 10 objects/2.8 GiB.                                     \n",
      "Copying file://./datasets/fleurs_ko_kr_features/dataset_dict.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/test/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/test/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/validation/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/test/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./datasets/fleurs_ko_kr_features/validation/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/validation/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/train/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/train/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_ko_kr_features/train/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "| [10/10 files][  2.6 GiB/  2.6 GiB] 100% Done  40.8 MiB/s ETA 00:00:00         \n",
      "Operation completed over 10 objects/2.6 GiB.                                     \n",
      "Copying file://./datasets/fleurs_he_il_features/dataset_dict.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/test/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/test/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/test/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://./datasets/fleurs_he_il_features/validation/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/validation/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_he_il_features/train/dataset.arrow [Content-Type=application/octet-stream]...\n",
      "Copying file://./datasets/fleurs_he_il_features/train/state.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/validation/dataset_info.json [Content-Type=application/json]...\n",
      "Copying file://./datasets/fleurs_he_il_features/train/dataset_info.json [Content-Type=application/json]...\n",
      "/ [10/10 files][  3.9 GiB/  3.9 GiB] 100% Done  39.5 MiB/s ETA 00:00:00         \n",
      "Operation completed over 10 objects/3.9 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# removed -n to allow overwrite\n",
    "\n",
    "!gsutil -m cp -r./ datasets / fleurs_te_in_features gs: // capstone_datasets / fleurs / preprocess / fl_te_features\n",
    "!gsutil -m cp -r./ datasets / fleurs_ko_kr_features gs: // capstone_datasets / fleurs / preprocess / fl_ko_features\n",
    "!gsutil -m cp -r./ datasets / fleurs_he_il_features gs: // capstone_datasets / fleurs / preprocess / fl_iw_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from transformers import is_apex_available\n",
    "import torch\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def is_cuda_and_apex_available():\n",
    "    is_using_cuda = torch.cuda.is_available() and torch_device == \"cuda\"\n",
    "    return is_using_cuda and is_apex_available()\n",
    "\n",
    "use_fp16 = is_cuda_and_apex_available()\n",
    "\n",
    "print(use_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Preprocessed Data and Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from datasets import load_from_disk\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "langs = [\"ko_kr\", \"he_il\", \"te_in\"] # token for Fleurs dataset\n",
    "codes = {\"te_in\":'te', \"ko_kr\":'ko', \"he_il\":'iw'} # token for normalizer and processor decoder\n",
    "languages = {\"te_in\": 'Telugu', \"ko_kr\": 'Korean', \"he_il\": 'Hebrew'} # token for Whisper processor and tokenizer\n",
    "fleurs = {}\n",
    "models = {}\n",
    "collators = {}\n",
    "processors={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2307\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 226\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 382\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 3242\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 328\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 792\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    # load dataset from disk\n",
    "    fleurs[lang] = load_from_disk(f\"/home/alexandriaguo/datasets/fleurs_{lang}_features\")\n",
    "    print(fleurs[lang])\n",
    "\n",
    "    # load whisper processor\n",
    "    processors[lang] = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=languages[lang], task=\"transcribe\")\n",
    "\n",
    "    # initialize data collator\n",
    "    collators[lang] = utils.DataCollatorSpeechSeq2SeqWithPadding(processor=processors[lang])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# universal training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/sivan\",\n",
    "    per_device_train_batch_size=16,  # originally 16\n",
    "    gradient_accumulation_steps=1,  # originally 1, increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # original True\n",
    "    group_by_length=False,  # set true if length is specified in dataset\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,  # set true to push trained model to HF\n",
    "    disable_tqdm=False,  # set false to see progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ko_kr starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2307\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3017' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3017/4000 4:09:55 < 1:21:29, 0.20 it/s, Epoch 20.80/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.420535</td>\n",
       "      <td>28.889558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.477162</td>\n",
       "      <td>28.137225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.506354</td>\n",
       "      <td>29.160397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 226\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['택배 회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종정 업무서류 긴급수리를 위한 상품이나 예비 부품의 경우에는 시간 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 첨중임으로 물로그작가는 비판을 피하기 위해 그 시기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙 아메리카에 서운 대부분의 물품은 이 나라의 면세로 들어갔습니다 ', '또한 의회는 주간의 세법과 관세를 무역할 권한이 없었습니다 '] ['택배회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종종 업무 서류 긴급 수리를 위한 상품이나 예비 부품의 경우에는 시간이 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙아메리카에서 온 대부분의 물품은 이 나라에 면세로 들어왔습니다 ', '또한 의회는 주 간의 세법과 관세를 무효화할 권한이 없었습니다 ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_ko_kr/checkpoint-1000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 226\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['택배 회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종정 업무서류 긴급수리를 위한 상품이나 예비 부품의 경우에는 시간 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 첨중임으로 물로그작가는 비판을 피하기 위해 그 시기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙 아메리카에 서운 대부분의 물품은 이 나라의 면세로 들어갔습니다 ', '또한 의회는 주간의 세법과 관세를 무유할 권한이 없었습니다 '] ['택배회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종종 업무 서류 긴급 수리를 위한 상품이나 예비 부품의 경우에는 시간이 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙아메리카에서 온 대부분의 물품은 이 나라에 면세로 들어왔습니다 ', '또한 의회는 주 간의 세법과 관세를 무효화할 권한이 없었습니다 ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_ko_kr/checkpoint-2000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 226\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['택배 회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종정 업무서류 긴급수리를 위한 상품이나 예비 부품의 경우에는 시간 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 첨중임으로 물로 그작가는 비판을 피하기 위해 그 시기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙 아메리카에 서운 대부분의 물품은 이 나라의 면세로 들어갔습니다 ', '또한 의회는 주간의 세법과 관세를 무유할 권한이 없었습니다 '] ['택배회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종종 업무 서류 긴급 수리를 위한 상품이나 예비 부품의 경우에는 시간이 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙아메리카에서 온 대부분의 물품은 이 나라에 면세로 들어왔습니다 ', '또한 의회는 주 간의 세법과 관세를 무효화할 권한이 없었습니다 ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_ko_kr/checkpoint-3000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lang in langs:\n",
    "    print(f\"Training for {lang} starts...\")\n",
    "\n",
    "    # load pretrained model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # uncomment for evaluate original whisper base\n",
    "    # model.config.forced_decoder_ids = processors[lang].get_decoder_prompt_ids(language = codes[lang], task = \"transcribe\")\n",
    "\n",
    "    # redefine output path\n",
    "    training_args.output_dir = f\"/home/sivan/whisper_base_fl_{lang}\"\n",
    "\n",
    "    # specify metric for each language\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=languages[lang], task=\"transcribe\")\n",
    "    compute_metrics = utils.metrics(codes[lang], tokenizer)\n",
    "\n",
    "\n",
    "    # set trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=fleurs[lang][\"train\"],\n",
    "        eval_dataset=fleurs[lang][\"validation\"],\n",
    "        data_collator=collators[lang],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processors[lang].feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train() # comment to eval original whisper base\n",
    "\n",
    "    predict_results = trainer.predict(fleurs[lang][\"test\"], metric_key_prefix=\"test\")\n",
    "    metrics = predict_results.metrics\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "    trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ko_kr starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Loading model from /home/sivan/whisper_base_fl_ko_kr/checkpoint-3000.\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2307\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 20\n",
      "  Continuing training from global step 3000\n",
      "  Will skip the first 20 epochs then the first 100 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a5491cb33f4844a4db293ce8e7a808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 1:15:07, Epoch 27/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.518363</td>\n",
       "      <td>28.919651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 226\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['택배 회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종정 업무서류 긴급수리를 위한 상품이나 예비 부품의 경우에는 시간 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 첨중임으로 물로 그작가는 비판을 피하기 위해 그 시기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙 아메리카에 서운 대부분의 물품은 이 나라의 면세로 들어갔습니다 ', '또한 의회는 주간의 세법과 관세를 무유할 권한이 없었습니다 '] ['택배회사는 물건을 빨리 배달해 주는 대가로 많은 보수를 받습니다 종종 업무 서류 긴급 수리를 위한 상품이나 예비 부품의 경우에는 시간이 매우 중요합니다 ', '과학자들은 충돌로 인한 폭발이 거대했다고 말한다 ', '대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다 ', '아실지 모르겠지만 중앙아메리카에서 온 대부분의 물품은 이 나라에 면세로 들어왔습니다 ', '또한 의회는 주 간의 세법과 관세를 무효화할 권한이 없었습니다 ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_ko_kr/checkpoint-4000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/sivan/whisper_base_fl_ko_kr/checkpoint-2000 (score: 28.13722539873608).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 382\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['피라미드 사운드와 광선쇼는 이 지역에서 어린이들의 흥미를 가장 많이 끌어들이는 것들 중 하나입니다 ', ' 더 전통적인 규회는 부활절 주간의 토요일 밤에 부활 처리하자를 열며 여기에서 신도들이 종종 자정의 주의 부활을 축하합니다 ', '산을 둘러싸고 있는 맑고 아름다운 하늘 바뀌는 보이지 않는다 이 동굴안에서는 바깥 세상이 보이지도 들리지도 않는다 ', '장면들이 피라미드들 위에 비쳤고 다른 피라미드들에는 불이 밝혀졌다 ', '뇨의 병리와 행동 사이에 상관관계가 과학자들의 연구를 독부습니다 '] ['피라미드 사운드와 광선 쇼는 이 지역에서 어린이들의 흥미를 가장 많이 끌어들이는 것들 중 하나입니다 ', '더 전통적인 교회는 부활절 주간의 토요일 밤에 부활 철야제를 열며 여기에서 신도들이 종종 자정에 주의 부활을 축하합니다 ', '산을 둘러싸고 있는 맑고 아름다운 하늘밖에는 보이지 않는다 이 동굴 안에서는 바깥세상이 보이지도 들리지도 않는다 ', '장면들이 피라미드들 위에 비쳤고 다른 피라미드들에는 불이 밝혀졌다 ', '뇌 병리와 행동 사이의 상관관계가 과학자들의 연구를 돕습니다 ']\n",
      "***** test metrics *****\n",
      "  test_loss               =     0.5271\n",
      "  test_runtime            = 0:04:06.12\n",
      "  test_samples_per_second =      1.552\n",
      "  test_steps_per_second   =      0.049\n",
      "  test_wer                =    28.3924\n",
      "Training for he_il starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/vocab.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file merges.txt from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/merges.txt\n",
      "loading file normalizer.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/normalizer.json\n",
      "loading file added_tokens.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/tokenizer_config.json\n",
      "Adding <|endoftext|> to the vocabulary\n",
      "Adding <|startoftranscript|> to the vocabulary\n",
      "Adding <|en|> to the vocabulary\n",
      "Adding <|zh|> to the vocabulary\n",
      "Adding <|de|> to the vocabulary\n",
      "Adding <|es|> to the vocabulary\n",
      "Adding <|ru|> to the vocabulary\n",
      "Adding <|ko|> to the vocabulary\n",
      "Adding <|fr|> to the vocabulary\n",
      "Adding <|ja|> to the vocabulary\n",
      "Adding <|pt|> to the vocabulary\n",
      "Adding <|tr|> to the vocabulary\n",
      "Adding <|pl|> to the vocabulary\n",
      "Adding <|ca|> to the vocabulary\n",
      "Adding <|nl|> to the vocabulary\n",
      "Adding <|ar|> to the vocabulary\n",
      "Adding <|sv|> to the vocabulary\n",
      "Adding <|it|> to the vocabulary\n",
      "Adding <|id|> to the vocabulary\n",
      "Adding <|hi|> to the vocabulary\n",
      "Adding <|fi|> to the vocabulary\n",
      "Adding <|vi|> to the vocabulary\n",
      "Adding <|iw|> to the vocabulary\n",
      "Adding <|uk|> to the vocabulary\n",
      "Adding <|el|> to the vocabulary\n",
      "Adding <|ms|> to the vocabulary\n",
      "Adding <|cs|> to the vocabulary\n",
      "Adding <|ro|> to the vocabulary\n",
      "Adding <|da|> to the vocabulary\n",
      "Adding <|hu|> to the vocabulary\n",
      "Adding <|ta|> to the vocabulary\n",
      "Adding <|no|> to the vocabulary\n",
      "Adding <|th|> to the vocabulary\n",
      "Adding <|ur|> to the vocabulary\n",
      "Adding <|hr|> to the vocabulary\n",
      "Adding <|bg|> to the vocabulary\n",
      "Adding <|lt|> to the vocabulary\n",
      "Adding <|la|> to the vocabulary\n",
      "Adding <|mi|> to the vocabulary\n",
      "Adding <|ml|> to the vocabulary\n",
      "Adding <|cy|> to the vocabulary\n",
      "Adding <|sk|> to the vocabulary\n",
      "Adding <|te|> to the vocabulary\n",
      "Adding <|fa|> to the vocabulary\n",
      "Adding <|lv|> to the vocabulary\n",
      "Adding <|bn|> to the vocabulary\n",
      "Adding <|sr|> to the vocabulary\n",
      "Adding <|az|> to the vocabulary\n",
      "Adding <|sl|> to the vocabulary\n",
      "Adding <|kn|> to the vocabulary\n",
      "Adding <|et|> to the vocabulary\n",
      "Adding <|mk|> to the vocabulary\n",
      "Adding <|br|> to the vocabulary\n",
      "Adding <|eu|> to the vocabulary\n",
      "Adding <|is|> to the vocabulary\n",
      "Adding <|hy|> to the vocabulary\n",
      "Adding <|ne|> to the vocabulary\n",
      "Adding <|mn|> to the vocabulary\n",
      "Adding <|bs|> to the vocabulary\n",
      "Adding <|kk|> to the vocabulary\n",
      "Adding <|sq|> to the vocabulary\n",
      "Adding <|sw|> to the vocabulary\n",
      "Adding <|gl|> to the vocabulary\n",
      "Adding <|mr|> to the vocabulary\n",
      "Adding <|pa|> to the vocabulary\n",
      "Adding <|si|> to the vocabulary\n",
      "Adding <|km|> to the vocabulary\n",
      "Adding <|sn|> to the vocabulary\n",
      "Adding <|yo|> to the vocabulary\n",
      "Adding <|so|> to the vocabulary\n",
      "Adding <|af|> to the vocabulary\n",
      "Adding <|oc|> to the vocabulary\n",
      "Adding <|ka|> to the vocabulary\n",
      "Adding <|be|> to the vocabulary\n",
      "Adding <|tg|> to the vocabulary\n",
      "Adding <|sd|> to the vocabulary\n",
      "Adding <|gu|> to the vocabulary\n",
      "Adding <|am|> to the vocabulary\n",
      "Adding <|yi|> to the vocabulary\n",
      "Adding <|lo|> to the vocabulary\n",
      "Adding <|uz|> to the vocabulary\n",
      "Adding <|fo|> to the vocabulary\n",
      "Adding <|ht|> to the vocabulary\n",
      "Adding <|ps|> to the vocabulary\n",
      "Adding <|tk|> to the vocabulary\n",
      "Adding <|nn|> to the vocabulary\n",
      "Adding <|mt|> to the vocabulary\n",
      "Adding <|sa|> to the vocabulary\n",
      "Adding <|lb|> to the vocabulary\n",
      "Adding <|my|> to the vocabulary\n",
      "Adding <|bo|> to the vocabulary\n",
      "Adding <|tl|> to the vocabulary\n",
      "Adding <|mg|> to the vocabulary\n",
      "Adding <|as|> to the vocabulary\n",
      "Adding <|tt|> to the vocabulary\n",
      "Adding <|haw|> to the vocabulary\n",
      "Adding <|ln|> to the vocabulary\n",
      "Adding <|ha|> to the vocabulary\n",
      "Adding <|ba|> to the vocabulary\n",
      "Adding <|jw|> to the vocabulary\n",
      "Adding <|su|> to the vocabulary\n",
      "Adding <|translate|> to the vocabulary\n",
      "Adding <|transcribe|> to the vocabulary\n",
      "Adding <|startoflm|> to the vocabulary\n",
      "Adding <|startofprev|> to the vocabulary\n",
      "Adding <|nocaptions|> to the vocabulary\n",
      "Adding <|notimestamps|> to the vocabulary\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid checkpoint found in output directory (/home/sivan/whisper_base_fl_he_il)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:34\u001B[0m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1501\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1499\u001B[0m     resume_from_checkpoint \u001B[38;5;241m=\u001B[39m get_last_checkpoint(args\u001B[38;5;241m.\u001B[39moutput_dir)\n\u001B[1;32m   1500\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resume_from_checkpoint \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1501\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo valid checkpoint found in output directory (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs\u001B[38;5;241m.\u001B[39moutput_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resume_from_checkpoint \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[1;32m   1504\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_from_checkpoint(resume_from_checkpoint)\n",
      "\u001B[0;31mValueError\u001B[0m: No valid checkpoint found in output directory (/home/sivan/whisper_base_fl_he_il)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# continue training\n",
    "langs = [\"ko_kr\", \"he_il\", \"te_in\"]\n",
    "for lang in langs:\n",
    "    print(f\"Training for {lang} starts...\")\n",
    "\n",
    "    # load pretrained model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # uncomment for evaluate original whisper base\n",
    "    # model.config.forced_decoder_ids = processors[lang].get_decoder_prompt_ids(language = codes[lang], task = \"transcribe\")\n",
    "\n",
    "    # redefine output path\n",
    "    training_args.output_dir = f\"/home/sivan/whisper_base_fl_{lang}\"\n",
    "\n",
    "    # specify metric for each language\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=languages[lang], task=\"transcribe\")\n",
    "    compute_metrics = utils.metrics(codes[lang], tokenizer)\n",
    "\n",
    "\n",
    "    # set trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=fleurs[lang][\"train\"],\n",
    "        eval_dataset=fleurs[lang][\"validation\"],\n",
    "        data_collator=collators[lang],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processors[lang].feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint = True) # comment to eval only\n",
    "\n",
    "    predict_results = trainer.predict(fleurs[lang][\"test\"], metric_key_prefix=\"test\")\n",
    "    metrics = predict_results.metrics\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "    trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for he_il starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3242\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 5:16:04, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Wer Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.882068</td>\n",
       "      <td>62.346019</td>\n",
       "      <td>59.526843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>1.043117</td>\n",
       "      <td>65.416437</td>\n",
       "      <td>62.274795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>1.190348</td>\n",
       "      <td>68.284611</td>\n",
       "      <td>64.585987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.240360</td>\n",
       "      <td>66.317338</td>\n",
       "      <td>63.166515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 328\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הדת השלטת במולדובה היא נצרות אורטודוגסית.', 'ברגשווצים האזריים עשכי החזרה היא נקשה מהרגיל.', 'מכיר הראשונים של המחלה הברונל הוכיחית דווחו בסוף יהיו לי.', 'חדת השולטת במולדו והאנצרות אורטוגורקסית (אורטוגורקסית).', '\"זוהי אינה בירום מורכבת, אך נעימה ומירועננת. הבירה המקומית הנוספת נקראית \"\"נמברוואן\"\" הבירה המקומית האיקרית אימנטה.\"'] ['הדת השלטת במולדובה היא נצרות אורתודוקסית.', 'ברגע שיוצאים מהזרם, השחייה חזרה אינה קשה מהרגיל.', 'מקריה הראשונים של המחלה בעונה הנוכחית דווחו בסוף יולי.', 'הדת השלטת במולדובה היא נצרות אורתודוקסית.', '\"הבירה המקומית העיקרית היא \\'Number One\\', זוהי אינה בירה מורכבת, אך נעימה ומרעננת. הבירה המקומית הנוספת נקראת \"\"Manta\"\".\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_he_il/checkpoint-1000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_he_il/checkpoint-1000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_he_il/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_he_il/checkpoint-1000/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 328\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הדת השלטת במונתובה היא נצרות אורטודוגסית.', 'ברגש הוציא מהאזרים עשכי החזרה עינה קשה מהרגיל.', 'מקיון ראשונים של המחלה בעולם הוכיחי דווחו בסוף יולי.', 'חדת השולטת במולדובה הנצרות הוטוגורקסית (אוטוגורקסית).', '\"זוהי אינה בירה מורכבת, אך נעימה ומירועננת. הבירה המקומית הנוספת נקרית נ\"\"מ (1) \"\"הבירה המקומית העיקרית אימנטה\"\".\"'] ['הדת השלטת במולדובה היא נצרות אורתודוקסית.', 'ברגע שיוצאים מהזרם, השחייה חזרה אינה קשה מהרגיל.', 'מקריה הראשונים של המחלה בעונה הנוכחית דווחו בסוף יולי.', 'הדת השלטת במולדובה היא נצרות אורתודוקסית.', '\"הבירה המקומית העיקרית היא \\'Number One\\', זוהי אינה בירה מורכבת, אך נעימה ומרעננת. הבירה המקומית הנוספת נקראת \"\"Manta\"\".\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_he_il/checkpoint-2000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_he_il/checkpoint-2000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_he_il/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_he_il/checkpoint-2000/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 328\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הדת השלטת במולדובה היא נצרות אורטודוגסית.', 'ברגשווצים האזרים עשכיח עזרה העינה קשה מהרגיל.', 'מקיון ראשונים של המחלה בעונה על הוכיחית, דווחו בסוף יולי.', 'חדת השולטת במולדובה הנצות אותו תוגרק סית, אותו דוקסית.', '\"זוהי הן הבירה מורכבת, אך נעימה ומירה עננת. הבירה המקומית הנוספת נקרית \"נמברואן\"\", הבירה המקומית האיקרית אימנטה.\"'] ['הדת השלטת במולדובה היא נצרות אורתודוקסית.', 'ברגע שיוצאים מהזרם, השחייה חזרה אינה קשה מהרגיל.', 'מקריה הראשונים של המחלה בעונה הנוכחית דווחו בסוף יולי.', 'הדת השלטת במולדובה היא נצרות אורתודוקסית.', '\"הבירה המקומית העיקרית היא \\'Number One\\', זוהי אינה בירה מורכבת, אך נעימה ומרעננת. הבירה המקומית הנוספת נקראת \"\"Manta\"\".\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_he_il/checkpoint-3000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_he_il/checkpoint-3000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_he_il/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_he_il/checkpoint-3000/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 328\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הדת השלטת במולדובה היא נצרות אורטודוקסית.', 'ברגשווצים האזרים השחייך אזהה אינה קשה מהרגיל.', 'מקיון השונים של המחלה בעונה על הוכיחי, דוברו בסוף יולי.', 'חדת השולטת במולדובה הנצות אותו תוגרק סית, אותו דוקסית.', '\"זוהי הן הבירה מורכבת, אך נעימה ומירה הננת.\" הבירה המקומית הנוספת נקרית \"נמברוואן\"\", הבירה המקומית האיקרית אימנטה.\"'] ['הדת השלטת במולדובה היא נצרות אורתודוקסית.', 'ברגע שיוצאים מהזרם, השחייה חזרה אינה קשה מהרגיל.', 'מקריה הראשונים של המחלה בעונה הנוכחית דווחו בסוף יולי.', 'הדת השלטת במולדובה היא נצרות אורתודוקסית.', '\"הבירה המקומית העיקרית היא \\'Number One\\', זוהי אינה בירה מורכבת, אך נעימה ומרעננת. הבירה המקומית הנוספת נקראת \"\"Manta\"\".\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_he_il/checkpoint-4000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_he_il/checkpoint-4000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_he_il/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_he_il/checkpoint-4000/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/sivan/whisper_base_fl_he_il/checkpoint-1000 (score: 62.346019488876635).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 792\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['בדרך כלל תמיד שום אם את כלות התיירים והמוכרים סיבור הכל והאור הוא מאמוש כמו סיבורים.', 'זודרח השובה להבדית במספר פעדים והצעמים.', 'הגובה המינימה למטחת לגשר הוא 5,000 מטר הבנייה היסטיימה באוגוסט 2001 והוא נפתח לתנוע רק במרץ 2010.', \"הזמר רג'ו קנדלואל ליוואטו, צפן בוג בהג'ן כמו כן לקסינג הציג את '.\", 'אבל הרבה דברים את סלצי פורים מדעי, נהים כמו דינוזאורי.'] ['בדרך כלל תמיד שומעים את קולות התיירים והמוכרים. סיפור הקול והאור הוא ממש כמו ספר סיפורים.', 'זו דרך חשובה להבדיל בין מספר פעלים ועצמים.', 'הגובה המינימלי מתחת לגשר הוא 15 מטר. הבנייה הסתיימה באוגוסט 2011, והוא נפתח לתנועה רק במרץ 2017.', \"כמו כן, לאקה סינג הציג את chhappan bhog bhajan. הזמר ראג'ו קנדלוואל ליווה אותו.\", 'אבל הרבה דברים אצל ציפורים עדיין נראים כמו דינוזאורים.']\n",
      "***** test metrics *****\n",
      "  test_loss               =     0.8387\n",
      "  test_runtime            = 0:12:20.80\n",
      "  test_samples_per_second =      1.069\n",
      "  test_steps_per_second   =      0.034\n",
      "  test_wer                =    60.9634\n",
      "  test_wer_norm           =    57.9269\n",
      "Training for te_in starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": [\n",
      "    [\n",
      "      1,\n",
      "      50259\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      50359\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      50363\n",
      "    ]\n",
      "  ],\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [\n",
      "    1,\n",
      "    2,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    14,\n",
      "    25,\n",
      "    26,\n",
      "    27,\n",
      "    28,\n",
      "    29,\n",
      "    31,\n",
      "    58,\n",
      "    59,\n",
      "    60,\n",
      "    61,\n",
      "    62,\n",
      "    63,\n",
      "    90,\n",
      "    91,\n",
      "    92,\n",
      "    93,\n",
      "    359,\n",
      "    503,\n",
      "    522,\n",
      "    542,\n",
      "    873,\n",
      "    893,\n",
      "    902,\n",
      "    918,\n",
      "    922,\n",
      "    931,\n",
      "    1350,\n",
      "    1853,\n",
      "    1982,\n",
      "    2460,\n",
      "    2627,\n",
      "    3246,\n",
      "    3253,\n",
      "    3268,\n",
      "    3536,\n",
      "    3846,\n",
      "    3961,\n",
      "    4183,\n",
      "    4667,\n",
      "    6585,\n",
      "    6647,\n",
      "    7273,\n",
      "    9061,\n",
      "    9383,\n",
      "    10428,\n",
      "    10929,\n",
      "    11938,\n",
      "    12033,\n",
      "    12331,\n",
      "    12562,\n",
      "    13793,\n",
      "    14157,\n",
      "    14635,\n",
      "    15265,\n",
      "    15618,\n",
      "    16553,\n",
      "    16604,\n",
      "    18362,\n",
      "    18956,\n",
      "    20075,\n",
      "    21675,\n",
      "    22520,\n",
      "    26130,\n",
      "    26161,\n",
      "    26435,\n",
      "    28279,\n",
      "    29464,\n",
      "    31650,\n",
      "    32302,\n",
      "    32470,\n",
      "    36865,\n",
      "    42863,\n",
      "    47425,\n",
      "    49870,\n",
      "    50254,\n",
      "    50258,\n",
      "    50360,\n",
      "    50361,\n",
      "    50362\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "loading file vocab.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/vocab.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file merges.txt from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/merges.txt\n",
      "loading file normalizer.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/normalizer.json\n",
      "loading file added_tokens.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sivan/.cache/huggingface/hub/models--openai--whisper-base/snapshots/267437e6e7a1d6cb0846340910c19302aa1fd1fb/tokenizer_config.json\n",
      "Adding <|endoftext|> to the vocabulary\n",
      "Adding <|startoftranscript|> to the vocabulary\n",
      "Adding <|en|> to the vocabulary\n",
      "Adding <|zh|> to the vocabulary\n",
      "Adding <|de|> to the vocabulary\n",
      "Adding <|es|> to the vocabulary\n",
      "Adding <|ru|> to the vocabulary\n",
      "Adding <|ko|> to the vocabulary\n",
      "Adding <|fr|> to the vocabulary\n",
      "Adding <|ja|> to the vocabulary\n",
      "Adding <|pt|> to the vocabulary\n",
      "Adding <|tr|> to the vocabulary\n",
      "Adding <|pl|> to the vocabulary\n",
      "Adding <|ca|> to the vocabulary\n",
      "Adding <|nl|> to the vocabulary\n",
      "Adding <|ar|> to the vocabulary\n",
      "Adding <|sv|> to the vocabulary\n",
      "Adding <|it|> to the vocabulary\n",
      "Adding <|id|> to the vocabulary\n",
      "Adding <|hi|> to the vocabulary\n",
      "Adding <|fi|> to the vocabulary\n",
      "Adding <|vi|> to the vocabulary\n",
      "Adding <|iw|> to the vocabulary\n",
      "Adding <|uk|> to the vocabulary\n",
      "Adding <|el|> to the vocabulary\n",
      "Adding <|ms|> to the vocabulary\n",
      "Adding <|cs|> to the vocabulary\n",
      "Adding <|ro|> to the vocabulary\n",
      "Adding <|da|> to the vocabulary\n",
      "Adding <|hu|> to the vocabulary\n",
      "Adding <|ta|> to the vocabulary\n",
      "Adding <|no|> to the vocabulary\n",
      "Adding <|th|> to the vocabulary\n",
      "Adding <|ur|> to the vocabulary\n",
      "Adding <|hr|> to the vocabulary\n",
      "Adding <|bg|> to the vocabulary\n",
      "Adding <|lt|> to the vocabulary\n",
      "Adding <|la|> to the vocabulary\n",
      "Adding <|mi|> to the vocabulary\n",
      "Adding <|ml|> to the vocabulary\n",
      "Adding <|cy|> to the vocabulary\n",
      "Adding <|sk|> to the vocabulary\n",
      "Adding <|te|> to the vocabulary\n",
      "Adding <|fa|> to the vocabulary\n",
      "Adding <|lv|> to the vocabulary\n",
      "Adding <|bn|> to the vocabulary\n",
      "Adding <|sr|> to the vocabulary\n",
      "Adding <|az|> to the vocabulary\n",
      "Adding <|sl|> to the vocabulary\n",
      "Adding <|kn|> to the vocabulary\n",
      "Adding <|et|> to the vocabulary\n",
      "Adding <|mk|> to the vocabulary\n",
      "Adding <|br|> to the vocabulary\n",
      "Adding <|eu|> to the vocabulary\n",
      "Adding <|is|> to the vocabulary\n",
      "Adding <|hy|> to the vocabulary\n",
      "Adding <|ne|> to the vocabulary\n",
      "Adding <|mn|> to the vocabulary\n",
      "Adding <|bs|> to the vocabulary\n",
      "Adding <|kk|> to the vocabulary\n",
      "Adding <|sq|> to the vocabulary\n",
      "Adding <|sw|> to the vocabulary\n",
      "Adding <|gl|> to the vocabulary\n",
      "Adding <|mr|> to the vocabulary\n",
      "Adding <|pa|> to the vocabulary\n",
      "Adding <|si|> to the vocabulary\n",
      "Adding <|km|> to the vocabulary\n",
      "Adding <|sn|> to the vocabulary\n",
      "Adding <|yo|> to the vocabulary\n",
      "Adding <|so|> to the vocabulary\n",
      "Adding <|af|> to the vocabulary\n",
      "Adding <|oc|> to the vocabulary\n",
      "Adding <|ka|> to the vocabulary\n",
      "Adding <|be|> to the vocabulary\n",
      "Adding <|tg|> to the vocabulary\n",
      "Adding <|sd|> to the vocabulary\n",
      "Adding <|gu|> to the vocabulary\n",
      "Adding <|am|> to the vocabulary\n",
      "Adding <|yi|> to the vocabulary\n",
      "Adding <|lo|> to the vocabulary\n",
      "Adding <|uz|> to the vocabulary\n",
      "Adding <|fo|> to the vocabulary\n",
      "Adding <|ht|> to the vocabulary\n",
      "Adding <|ps|> to the vocabulary\n",
      "Adding <|tk|> to the vocabulary\n",
      "Adding <|nn|> to the vocabulary\n",
      "Adding <|mt|> to the vocabulary\n",
      "Adding <|sa|> to the vocabulary\n",
      "Adding <|lb|> to the vocabulary\n",
      "Adding <|my|> to the vocabulary\n",
      "Adding <|bo|> to the vocabulary\n",
      "Adding <|tl|> to the vocabulary\n",
      "Adding <|mg|> to the vocabulary\n",
      "Adding <|as|> to the vocabulary\n",
      "Adding <|tt|> to the vocabulary\n",
      "Adding <|haw|> to the vocabulary\n",
      "Adding <|ln|> to the vocabulary\n",
      "Adding <|ha|> to the vocabulary\n",
      "Adding <|ba|> to the vocabulary\n",
      "Adding <|jw|> to the vocabulary\n",
      "Adding <|su|> to the vocabulary\n",
      "Adding <|translate|> to the vocabulary\n",
      "Adding <|transcribe|> to the vocabulary\n",
      "Adding <|startoflm|> to the vocabulary\n",
      "Adding <|startofprev|> to the vocabulary\n",
      "Adding <|nocaptions|> to the vocabulary\n",
      "Adding <|notimestamps|> to the vocabulary\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2302\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  24/4000 01:53 < 5:41:01, 0.19 it/s, Epoch 0.16/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (462) must match the size of tensor b (448) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:34\u001B[0m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1515\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1510\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1512\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1514\u001B[0m )\n\u001B[0;32m-> 1515\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1516\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1517\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1520\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1763\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1761\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   1762\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1763\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1766\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1767\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1768\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1769\u001B[0m ):\n\u001B[1;32m   1770\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1771\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2522\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2521\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2522\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2525\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2554\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2553\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2554\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2555\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2556\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1192\u001B[0m, in \u001B[0;36mWhisperForConditionalGeneration.forward\u001B[0;34m(self, input_features, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m decoder_input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m decoder_inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1188\u001B[0m         decoder_input_ids \u001B[38;5;241m=\u001B[39m shift_tokens_right(\n\u001B[1;32m   1189\u001B[0m             labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdecoder_start_token_id\n\u001B[1;32m   1190\u001B[0m         )\n\u001B[0;32m-> 1192\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1194\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1196\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1207\u001B[0m lm_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_out(outputs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m   1209\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:1061\u001B[0m, in \u001B[0;36mWhisperModel.forward\u001B[0;34m(self, input_features, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1054\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m BaseModelOutput(\n\u001B[1;32m   1055\u001B[0m         last_hidden_state\u001B[38;5;241m=\u001B[39mencoder_outputs[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m   1056\u001B[0m         hidden_states\u001B[38;5;241m=\u001B[39mencoder_outputs[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1057\u001B[0m         attentions\u001B[38;5;241m=\u001B[39mencoder_outputs[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1058\u001B[0m     )\n\u001B[1;32m   1060\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1061\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1062\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1063\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1064\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1066\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1067\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1070\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1071\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1072\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1073\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1075\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n\u001B[1;32m   1076\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m decoder_outputs \u001B[38;5;241m+\u001B[39m encoder_outputs\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/whisper/modeling_whisper.py:868\u001B[0m, in \u001B[0;36mWhisperDecoder.forward\u001B[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    865\u001B[0m \u001B[38;5;66;03m# embed positions\u001B[39;00m\n\u001B[1;32m    866\u001B[0m positions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_positions(input_ids, past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length)\n\u001B[0;32m--> 868\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43minputs_embeds\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpositions\u001B[49m\n\u001B[1;32m    869\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mdropout(hidden_states, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m    871\u001B[0m \u001B[38;5;66;03m# decoder layers\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (462) must match the size of tensor b (448) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# continue training\n",
    "langs = [\"he_il\", \"te_in\"]\n",
    "for lang in langs:\n",
    "    print(f\"Training for {lang} starts...\")\n",
    "\n",
    "    # load pretrained model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # uncomment for evaluate original whisper base\n",
    "    # model.config.forced_decoder_ids = processors[lang].get_decoder_prompt_ids(language = codes[lang], task = \"transcribe\")\n",
    "\n",
    "    # redefine output path\n",
    "    training_args.output_dir = f\"/home/sivan/whisper_base_fl_{lang}\"\n",
    "\n",
    "    # specify metric for each language\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=languages[lang], task=\"transcribe\")\n",
    "    compute_metrics = utils.metrics(codes[lang], tokenizer)\n",
    "\n",
    "\n",
    "    # set trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=fleurs[lang][\"train\"],\n",
    "        eval_dataset=fleurs[lang][\"validation\"],\n",
    "        data_collator=collators[lang],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processors[lang].feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train() # comment to eval only\n",
    "\n",
    "    predict_results = trainer.predict(fleurs[lang][\"test\"], metric_key_prefix=\"test\")\n",
    "    metrics = predict_results.metrics\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "    trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Telugu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examine raw data and  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 20:58:10.349806: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 20:58:10.565137: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 20:58:11.181487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-15 20:58:11.181587: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-15 20:58:11.181598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language='Telugu', task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fleurs (/home/sivan/.cache/huggingface/datasets/google___fleurs/te_in/2.0.0/aabb39fb29739c495517ac904e2886819b6e344702f0a5b5283cb178b087c94a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edef8e2e554b4dda9dede94caade0460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'num_samples', 'path', 'audio', 'transcription', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n",
      "{'id': 1436, 'num_samples': 275520, 'path': '/home/sivan/.cache/huggingface/datasets/downloads/extracted/638d0abfed4ef9ee8ed070f8b584c3492ed5c8d4ee54a323529c610cca3e6c91/te_in/audio/train/16034248050795652008.wav', 'audio': {'path': '16034248050795652008.wav', 'array': array([ 0.        ,  0.        ,  0.        , ...,  0.01135588,\n",
      "        0.00194162, -0.00869256]), 'sampling_rate': 16000}, 'transcription': 'కాసినోలు సాధారణంగా అతిధులు ఖర్చు పెట్టిన సమయాన్ని మరియు డబ్బును గరిష్టం చేయడానికి అనేక ప్రయత్నాలు చేస్తాయి కిటికీలు మరియు గడియారాలు సాధారణంగా లోపముగా ఉంటాయి మరియు నిష్క్రమణలను కనుగొనడం కష్టం', 'raw_transcription': 'కాసినోలు సాధారణంగా అతిధులు ఖర్చు పెట్టిన సమయాన్ని మరియు డబ్బును గరిష్టం చేయడానికి అనేక ప్రయత్నాలు చేస్తాయి. కిటికీలు మరియు గడియారాలు సాధారణంగా లోపముగా ఉంటాయి, మరియు నిష్క్రమణలను కనుగొనడం కష్టం.', 'gender': 1, 'lang_id': 88, 'language': 'Telugu', 'lang_group_id': 4}\n"
     ]
    }
   ],
   "source": [
    "fleurs_te= DatasetDict(load_dataset('google/fleurs', 'te_in'))\n",
    "print(fleurs_te)\n",
    "print(fleurs_te['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275520\n"
     ]
    }
   ],
   "source": [
    "print(len(fleurs_te['train'][0]['audio']['array']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 కాసినోలు సాధారణంగా అతిధులు ఖర్చు పెట్టిన సమయాన్ని మరియు డబ్బును గరిష్టం చేయడానికి అనేక ప్రయత్నాలు చేస్తాయి. కిటికీలు మరియు గడియారాలు సాధారణంగా లోపముగా ఉంటాయి, మరియు నిష్క్రమణలను కనుగొనడం కష్టం.\n",
      "Decoded w/ special:    <|startoftranscript|><|te|><|transcribe|><|notimestamps|>కాసినోలు సాధారణంగా అతిధులు ఖర్చు పెట్టిన సమయాన్ని మరియు డబ్బును గరిష్టం చేయడానికి అనేక ప్రయత్నాలు చేస్తాయి. కిటికీలు మరియు గడియారాలు సాధారణంగా లోపముగా ఉంటాయి, మరియు నిష్క్రమణలను కనుగొనడం కష్టం.<|endoftext|>\n",
      "Decoded w/out special: కాసినోలు సాధారణంగా అతిధులు ఖర్చు పెట్టిన సమయాన్ని మరియు డబ్బును గరిష్టం చేయడానికి అనేక ప్రయత్నాలు చేస్తాయి. కిటికీలు మరియు గడియారాలు సాధారణంగా లోపముగా ఉంటాయి, మరియు నిష్క్రమణలను కనుగొనడం కష్టం.\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = fleurs_te[\"train\"][0][\"raw_transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examine preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DURATION_IN_SECONDS = 30.0\n",
    "max_input_length = MAX_DURATION_IN_SECONDS * 16000\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute input length\n",
    "    batch[\"input_length\"] = len(batch[\"audio\"]['array'])\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], \n",
    "                                                truncation=True,\n",
    "                                                max_length=max_input_length\n",
    "                                                sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    \n",
    "    batch['feature_length'] = len(batch['input_features'])\n",
    "    \n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"raw_transcription\"], \n",
    "                                truncation=True,\n",
    "                                max_length=448).input_ids\n",
    "    \n",
    "    batch['norm'] = batch['transcription']\n",
    "\n",
    "    # compute labels length\n",
    "    batch[\"labels_length\"] = len(batch['labels'])\n",
    "    return batch\n",
    "\n",
    "def filter_inputs(input_length):\n",
    "    \"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
    "    return 0 < input_length < max_input_length\n",
    "\n",
    "def filter_labels(labels_length):\n",
    "    \"\"\"Filter empty label sequences\"\"\"\n",
    "    return 0 < labels_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bb538281ca4ee3b30812665e28aae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/576 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea44138043b4c5dafe2f86c0f5d1d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/576 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa407f61e974daea07b8760d73b5653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/575 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ce2d31392d4658956f56a5240fb6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/575 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17845604ad1745eb80bda7b613455c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/78 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097363133c6e416583680b4102eb1987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/78 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf065289e394a99ae4bed5ba9d18682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/78 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d78dc9ab7e842d2b0de55ca241a664d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/77 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a03e5dd46640d3b0d5c63a2172f309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28ee9c494374c569e9fd91396544e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ebb84615944ff18a52c36deb87bdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305e377fb2d942638c1967066b5620d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/118 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 1.85 s, total: 15.8 s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fleurs_te_pre = fleurs_te.map(prepare_dataset,\n",
    "                          remove_columns=fleurs_te.column_names[\"train\"], \n",
    "                          num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleurs_te_pre.save_to_disk('/home/sivan/datasets/fl_te_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 21:09:37.512402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 21:09:37.755330: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 21:09:38.959555: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-15 21:09:38.959685: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-15 21:09:38.959696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# from here\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_length', 'input_features', 'feature_length', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_length', 'input_features', 'feature_length', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_length', 'input_features', 'feature_length', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "fleurs_te_pre = load_from_disk(\"/home/sivan/datasets/fl_te_features\")\n",
    "print(fleurs_te_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(set(fleurs_te_pre['train']['input_length']))\n",
    "# print(set(fleurs_te_pre['train']['labels_length']))\n",
    "print(np.array(fleurs_te_pre['train'][\"input_features\"][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sivan/datasets/fl_te_features/train/cache-17d7c4e8c928aa5f.arrow\n",
      "Loading cached processed dataset at /home/sivan/datasets/fl_te_features/validation/cache-81671aa4d2208db4.arrow\n",
      "Loading cached processed dataset at /home/sivan/datasets/fl_te_features/test/cache-5e169a99e548838a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 ms, sys: 4.69 ms, total: 15.5 ms\n",
      "Wall time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fleurs_te_fil_in = fleurs_te.filter(filter_inputs, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sivan/datasets/fl_te_features/train/cache-d0ead2b2abea5683.arrow\n",
      "Loading cached processed dataset at /home/sivan/datasets/fl_te_features/validation/cache-bd49341be8d55e4a.arrow\n",
      "Loading cached processed dataset at /home/sivan/datasets/fl_te_features/test/cache-e3cdb938dcdda490.arrow\n"
     ]
    }
   ],
   "source": [
    "fleurs_te_filter = fleurs_te_fil_in.filter(filter_labels, input_columns=[\"labels_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 2302\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 472\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 2297\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 310\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 471\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 2297\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 310\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_length', 'input_features', 'labels', 'norm', 'labels_length'],\n",
      "        num_rows: 471\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fleurs_te)\n",
    "print(fleurs_te_fil_in)\n",
    "print(fleurs_te_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd12f4bedcf4e6793060550ce815d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fleurs_te_filter.save_to_disk('/home/sivan/datasets/fl_te_features_filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 21:11:58.161544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 21:11:58.436485: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 21:11:59.777292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-15 21:11:59.777409: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-15 21:11:59.777419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from datasets import load_from_disk\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language='Telugu', task=\"transcribe\")\n",
    "collator = utils.DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "fleurs_te = load_from_disk(\"/home/sivan/datasets/fl_te_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# universal training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/sivan/whisper_base_fl_te_in\",\n",
    "    per_device_train_batch_size=16,  # originally 16\n",
    "    gradient_accumulation_steps=1,  # originally 1, increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # original True\n",
    "    group_by_length=False,  # set true if length is specified in dataset\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,  # set true to push trained model to HF\n",
    "    disable_tqdm=False,  # set false to see progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: feature_length, input_length, labels_length, norm. If feature_length, input_length, labels_length, norm are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/sivan/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2302\n",
      "  Num Epochs = 28\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4000\n",
      "  Number of trainable parameters = 72593920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 3:19:38, Epoch 27/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Wer Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.281029</td>\n",
       "      <td>71.452421</td>\n",
       "      <td>37.764460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.430765</td>\n",
       "      <td>71.869783</td>\n",
       "      <td>38.840349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.550151</td>\n",
       "      <td>72.057596</td>\n",
       "      <td>38.797025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.584290</td>\n",
       "      <td>71.994992</td>\n",
       "      <td>38.558741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: feature_length, input_length, labels_length, norm. If feature_length, input_length, labels_length, norm are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 311\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['వర్చువల్ ఫ్యూయిల్ ట్ర్ప్స్ తో టెక్నాలిచీ పరిష్కారాన్ని అందిస్తుంది, విద్యర్దులు మ్యూజియం కలాకండాలను చూడవచ్చు, ఆక్వేర�', 'ప్లాటింగ్ విస్లేషన ఫలితం పద్రుక్ పబ్లిక్ వెబ్ సైట్ లో పోస్ట్ చేయబడుతుంది.', 'స్థానికదికార్లు లాండ్ చుక్ట్టు ప్రక్కల నివసిత్లను ఇండోర్లో ఉన్నమని ఏర్కన్నీషణర్లను మర్చేయలని మరియు కొలాయిల నిర్ తాగవ', 'చాలా సందర్భాలో విదేశాల్లో ఒక గ్యాబ్ ఏర్కోర్స్ లో చారడం అలనం మొల్నా మీ స్వంత దేశంలో తిరిగి ఉన్నతు విద్యకు వెళ్లే మీ యవకా', 'సంస్థ సుజనాత్మకంగా ఉండటానికి ముందు, నాయకత్తం సుజనాత్మకత యొక్క సంస్కృతిని లగే పంచుకునే గ్ఞాలాన్ని మరియు సంస్థాగత అభ్యస'] ['వర్చువల్ ఫీల్డ్ ట్రిప్స్ తో టెక్నాలజీ పరిష్కారాన్ని అందిస్తుంది. విద్యార్థులు మ్యూజియం కళాఖండాలను చూడవచ్చు, అక్వేరియంను సందర్శించవచ్చు లేదా వాళ్ళ  క్లాసులో కూర్చుని అందమైన కళను ఆరాధించవచ్చు.', 'ప్లాటింగ్ విశ్లేషణ ఫలితం పబ్లిక్ వెబ్\\u200cసైట్\\u200cలో పోస్ట్ చేయబడుతుంది.', 'స్థానిక అధికారులు ప్లాంట్ చుట్టుప్రక్కల నివాసితులను ఇండోర్ లో ఉండమని, ఎయిర్-కండిషనర్లను ఆఫ్ చేయాలని మరియు కుళాయి నీరు తాగవద్దని హెచ్చరిస్తున్నారు.', 'చాలా సందర్భాల్లో, విదేశాల్లో ఒక గ్యాప్ ఇయర్ కోర్సులో చేరడం వల్ల మీ స్వంత దేశంలో తిరిగి ఉన్నత విద్యకు వెళ్లే మీ అవకాశాలను మెరుగుపరుచుకోవచ్చు.', 'సంస్థ సృజనాత్మకంగా ఉండటానికి ముందు, నాయకత్వం సృజనాత్మకత యొక్క సంస్కృతిని అలాగే పంచుకునే జ్ఞానాన్ని మరియు సంస్థాగత అభ్యసనను రూపొందించాలి.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_te_in/checkpoint-1000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_te_in/checkpoint-1000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_te_in/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_te_in/checkpoint-1000/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: feature_length, input_length, labels_length, norm. If feature_length, input_length, labels_length, norm are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 311\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['వర్చవల్ ఫ్యేల్ ట్ర్ప్ తో టెక్నాలిచీ పరిషకారాన్ని అందిస్తుంది, విద్యర్థుల మ్యూజియం కలాకండాలకం చూడవచ్చు యాక్వేరియంను స�', 'ప్లాటింగ్, విస్లేసన ఫలితం 10 పబ్లిక్ వెబ్ సైట్ లో పోస్ట్రేయబడుతుంది.', 'స్థానిక అధికార్లు లాండ్ చుక్టు ప్రక్కల నివాసత్లను ఇండోర్లో ఉన్నమని ఏర్కన్నిషనర్లను ఓచేయలని మరియు కొలాయలనీర్ తాగవద్ని', 'చాలా సందర్భాలు విదేశాలో ఒక గ్యాబ్ ఏర్కోర్స్ లో చేరడం అల్నము, మీ స్వంత దేశంలో తిరిగి ఉన్నతు విద్యకు వెళ్లే మి యావకాశాలను', 'సంస్థ శుషనాత్మకంగా ఉండటానికి ముందు, నాయకత్తం శుషనాత్మ కత యొక్క సంస్కృతిన లగే పంచుకునే జ్ఞాలాన్ని మరియు సంస్థాగత అభియస�'] ['వర్చువల్ ఫీల్డ్ ట్రిప్స్ తో టెక్నాలజీ పరిష్కారాన్ని అందిస్తుంది. విద్యార్థులు మ్యూజియం కళాఖండాలను చూడవచ్చు, అక్వేరియంను సందర్శించవచ్చు లేదా వాళ్ళ  క్లాసులో కూర్చుని అందమైన కళను ఆరాధించవచ్చు.', 'ప్లాటింగ్ విశ్లేషణ ఫలితం పబ్లిక్ వెబ్\\u200cసైట్\\u200cలో పోస్ట్ చేయబడుతుంది.', 'స్థానిక అధికారులు ప్లాంట్ చుట్టుప్రక్కల నివాసితులను ఇండోర్ లో ఉండమని, ఎయిర్-కండిషనర్లను ఆఫ్ చేయాలని మరియు కుళాయి నీరు తాగవద్దని హెచ్చరిస్తున్నారు.', 'చాలా సందర్భాల్లో, విదేశాల్లో ఒక గ్యాప్ ఇయర్ కోర్సులో చేరడం వల్ల మీ స్వంత దేశంలో తిరిగి ఉన్నత విద్యకు వెళ్లే మీ అవకాశాలను మెరుగుపరుచుకోవచ్చు.', 'సంస్థ సృజనాత్మకంగా ఉండటానికి ముందు, నాయకత్వం సృజనాత్మకత యొక్క సంస్కృతిని అలాగే పంచుకునే జ్ఞానాన్ని మరియు సంస్థాగత అభ్యసనను రూపొందించాలి.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_te_in/checkpoint-2000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_te_in/checkpoint-2000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_te_in/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_te_in/checkpoint-2000/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: feature_length, input_length, labels_length, norm. If feature_length, input_length, labels_length, norm are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 311\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['వర్చవల్ ఫ్యయల్ ట్ర్లుట్రీప్ తో టెక్నాలిచీ పరిష్కారాన్ని అందిస్తుంది, విద్యార్థ్\\u200cలు మ్యూజియం కలాకండాలను చూడవచ్చు ఆక్వ�', 'ప్లాటింగ్, విస్లేసన ఫలితం 10 పబ్లిక్ వెబ్ సైట్ లో పోస్ట్రేయబడుతుంది.', 'స్థానిక అధికార్లు క్లాండ్ చుట్టు ప్రక్కల నివాసత్లను ఇండోర్లో ఉన్నమని ఏర్కన్నీషనర్లనో మర్చేయలని మరియు కొలాయలనీర్ తాగవ', 'చాలా సందర్భాలులో విదేశాలో ఒక గ్యాబ్ ఏర్కోర్స్ లో చారండం వల్నమీ స్వంత దేశంలో తిరిగి ఉన్నతు విద్యకు వెళ్డే మి యావకాశాలను', 'సంస్థ శ\\u0c75జనాత్మగం గా ఉండటానికి ముందు, నాయకత్తం శ\\u0c75జనాత్మ కత యొక్క సంస్కృతి ని లగే పంచుకునే జ్ఞాలాన్ని మరియు సంస్థగత అభ్య�'] ['వర్చువల్ ఫీల్డ్ ట్రిప్స్ తో టెక్నాలజీ పరిష్కారాన్ని అందిస్తుంది. విద్యార్థులు మ్యూజియం కళాఖండాలను చూడవచ్చు, అక్వేరియంను సందర్శించవచ్చు లేదా వాళ్ళ  క్లాసులో కూర్చుని అందమైన కళను ఆరాధించవచ్చు.', 'ప్లాటింగ్ విశ్లేషణ ఫలితం పబ్లిక్ వెబ్\\u200cసైట్\\u200cలో పోస్ట్ చేయబడుతుంది.', 'స్థానిక అధికారులు ప్లాంట్ చుట్టుప్రక్కల నివాసితులను ఇండోర్ లో ఉండమని, ఎయిర్-కండిషనర్లను ఆఫ్ చేయాలని మరియు కుళాయి నీరు తాగవద్దని హెచ్చరిస్తున్నారు.', 'చాలా సందర్భాల్లో, విదేశాల్లో ఒక గ్యాప్ ఇయర్ కోర్సులో చేరడం వల్ల మీ స్వంత దేశంలో తిరిగి ఉన్నత విద్యకు వెళ్లే మీ అవకాశాలను మెరుగుపరుచుకోవచ్చు.', 'సంస్థ సృజనాత్మకంగా ఉండటానికి ముందు, నాయకత్వం సృజనాత్మకత యొక్క సంస్కృతిని అలాగే పంచుకునే జ్ఞానాన్ని మరియు సంస్థాగత అభ్యసనను రూపొందించాలి.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_te_in/checkpoint-3000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_te_in/checkpoint-3000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_te_in/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_te_in/checkpoint-3000/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: feature_length, input_length, labels_length, norm. If feature_length, input_length, labels_length, norm are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 311\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['వర్చవల్ ఫ్యూల్ ట్ర్ప్ ట్రీప్ తో టెక్నాలిచీ పరిష్కారాన్ని అందిస్తుంది, విద్యార్థులు మ్యూజియం కలాకండాలను చూడవచ్చు ఆక్వ�', 'ప్లాటింగ్, విస్లేసన ఫలితం 10 పబ్లిక్ వెబ్ సైట్ లో పోస్ట్రేయబడుతుంది.', 'స్థానిక అధికార్లు క్లాండ్ చుట్టు ప్రాక్కల నివాసిత్లను ఇండోర్లో ఉన్నమని ఏర్కన్నీషనర్లనో మర్చేయలని మరియు కొలాయలనీర్ తా', 'చాలా సందర్భాలులో విదేశాలో ఒక గ్యాబ్ యేర్కోర్స్ లో చేరడం వల్నామి స్వంత దేశంలో తిరిగి ఉన్నతు విద్యకు వెళ్డే మి యావకాశాలన', 'సంస్థ శువసణాత్మకం గా ఉండటానికి ముందు, నాయికత్తం శువసణాత్మ కత యొక్క సంస్కృతి ని లగే పంచుకునే జ్ఞాలన్ని మరియు సంస్థగత అభ�'] ['వర్చువల్ ఫీల్డ్ ట్రిప్స్ తో టెక్నాలజీ పరిష్కారాన్ని అందిస్తుంది. విద్యార్థులు మ్యూజియం కళాఖండాలను చూడవచ్చు, అక్వేరియంను సందర్శించవచ్చు లేదా వాళ్ళ  క్లాసులో కూర్చుని అందమైన కళను ఆరాధించవచ్చు.', 'ప్లాటింగ్ విశ్లేషణ ఫలితం పబ్లిక్ వెబ్\\u200cసైట్\\u200cలో పోస్ట్ చేయబడుతుంది.', 'స్థానిక అధికారులు ప్లాంట్ చుట్టుప్రక్కల నివాసితులను ఇండోర్ లో ఉండమని, ఎయిర్-కండిషనర్లను ఆఫ్ చేయాలని మరియు కుళాయి నీరు తాగవద్దని హెచ్చరిస్తున్నారు.', 'చాలా సందర్భాల్లో, విదేశాల్లో ఒక గ్యాప్ ఇయర్ కోర్సులో చేరడం వల్ల మీ స్వంత దేశంలో తిరిగి ఉన్నత విద్యకు వెళ్లే మీ అవకాశాలను మెరుగుపరుచుకోవచ్చు.', 'సంస్థ సృజనాత్మకంగా ఉండటానికి ముందు, నాయకత్వం సృజనాత్మకత యొక్క సంస్కృతిని అలాగే పంచుకునే జ్ఞానాన్ని మరియు సంస్థాగత అభ్యసనను రూపొందించాలి.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/sivan/whisper_base_fl_te_in/checkpoint-4000\n",
      "Configuration saved in /home/sivan/whisper_base_fl_te_in/checkpoint-4000/config.json\n",
      "Model weights saved in /home/sivan/whisper_base_fl_te_in/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in /home/sivan/whisper_base_fl_te_in/checkpoint-4000/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/sivan/whisper_base_fl_te_in/checkpoint-1000 (score: 71.45242070116862).\n",
      "The following columns in the test set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: feature_length, input_length, labels_length, norm. If feature_length, input_length, labels_length, norm are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 472\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['అనెండవ సెట్ లో డెల్ పోట్రాకు ఆధిక్యం లభించిన కూడా ఈ సెట్ లో కూడా ఆరు ఆర్నించి ఆరికీ చేరుకున్న తర్వాత డైబ్రే గణివారియం అ�', 'హాథరైనవారిసంకే ఎంత ఎక్కువంటే సెన్ పీటస్ క్వేడ్ లు జరీ నంత క్రీలన్ చూగడానికి అందరికి సాధ్యపడలేదు.', 'కేవలం కొన్ని స్వల్ప భేదాలు తప్ప కాల్పనిక బృందాలు కూడా సంప్రదాయ బృందాలు వంటి నైకున్యం కలిగి ఉంటాయి.', 'ఏరోస్మిద్వారి పర్యటనలో మిగిలిన సంగీత కచేరీలను రద్దుచేసింది.', 'ఈ ప్రమాదం ఎత్తుగా ఉన్న పర్వత భూభాగంలో జరిగిన్నని ఇంకా శెత్రులు రాజేసిన అగ్ని ఫలితంగా జరిగిన్నని నమ్ముతారు.'] ['రెండవ సెట్\\u200cలో Del Potroకు ఆధిక్యం లభించినా కూడా, ఈ సెట్\\u200cలో కూడా 6-6కి చేరుకున్న తర్వాత టై బ్రేక్ అనివార్యం అయ్యింది.', \"హాజరైన వారి సంఖ్య ఎంత ఎక్కువంటే, St. Peter's స్క్వేర్\\u200cలో జరిగిన అంత్యక్రియలను చూడడానికి అందరికీ సాధ్యపడలేదు.\", 'కేవలం కొన్ని స్వల్ప బేధాలు తప్ప, కాల్పనిక బృందాలు కూడా సాంప్రదాయ బృందాల వంటి నైపుణ్యం కలిగి ఉంటాయి.', 'ఏరోస్మిత్ వారి పర్యటనలో మిగిలిన సంగీత కచేరీలను రద్దు చేసింది.', 'ఈ ప్రమాదం ఎత్తుగా ఉన్న పర్వత భూభాగంలో జరిగిందని, ఇంకా శత్రువులు రాజేసిన అగ్ని ఫలితంగా జరిగిందని నమ్ముతారు.']\n",
      "***** test metrics *****\n",
      "  test_loss               =     0.2943\n",
      "  test_runtime            = 0:05:47.51\n",
      "  test_samples_per_second =      1.358\n",
      "  test_steps_per_second   =      0.043\n",
      "  test_wer                =     71.493\n",
      "  test_wer_norm           =     40.339\n",
      "CPU times: user 3h 2min 40s, sys: 27min 54s, total: 3h 30min 35s\n",
      "Wall time: 3h 25min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load pretrained model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(\"cuda\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False\n",
    "\n",
    "# specify metric for each language\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Telugu\", task=\"transcribe\")\n",
    "compute_metrics = utils.metrics(\"iw\", tokenizer)\n",
    "\n",
    "\n",
    "# set trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=fleurs_te[\"train\"],\n",
    "    eval_dataset=fleurs_te[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "print(f\"Training starts...\")\n",
    "trainer.train() # comment to eval original whisper base\n",
    "\n",
    "predict_results = trainer.predict(fleurs_te[\"test\"], metric_key_prefix=\"test\")\n",
    "metrics = predict_results.metrics\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rememeber to save checkpoint models to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/test_results.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-4000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/all_results.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-1000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-3000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_ko_kr/checkpoint-2000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "| [34/34 files][  3.2 GiB/  3.2 GiB] 100% Done  90.3 MiB/s ETA 00:00:00         \n",
      "Operation completed over 34 objects/3.2 GiB.                                     \n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/all_results.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/test_results.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/config.json [Content-Type=application/json]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-4000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/rng_state.pth [Content-Type=application/octet-stream]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/runs/Nov15_21-12-05_t4/events.out.tfevents.1668546734.t4.2856.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-1000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-3000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/runs/Nov15_21-12-05_t4/1668546734.3549745/events.out.tfevents.1668546734.t4.2856.1 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/runs/Nov15_20-54-33_t4/events.out.tfevents.1668545717.t4.25036.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/runs/Nov15_20-54-33_t4/1668545717.8411813/events.out.tfevents.1668545717.t4.25036.1 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_te_in/checkpoint-2000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "| [38/38 files][  3.2 GiB/  3.2 GiB] 100% Done  57.9 MiB/s ETA 00:00:00         \n",
      "Operation completed over 38 objects/3.2 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# upload checkpoints to bucket too...\n",
    "# !gsutil -m cp -n -r /home/sivan/whisper_base_fl_cmn_hans_cn gs://capstone_datasets/fleurs/finetune/wshiper_base_fl_cmn_hans_cn\n",
    "!gsutil -m cp -n -r /home/sivan/whisper_base_fl_ko_kr gs://capstone_datasets/fleurs/finetune/wshiper_base_fl_ko_kr\n",
    "!gsutil -m cp -n -r /home/sivan/whisper_base_fl_te_in gs://capstone_datasets/fleurs/finetune/wshiper_base_fl_te_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/test_results.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/all_results.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-3000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-4000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/trainer_state.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-1000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/optimizer.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/rng_state.pth [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/preprocessor_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/scheduler.pt [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/training_args.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/sivan/whisper_base_fl_he_il/checkpoint-2000/trainer_state.json [Content-Type=application/json]...\n",
      "- [34/34 files][  3.2 GiB/  3.2 GiB] 100% Done 122.9 MiB/s ETA 00:00:00         \n",
      "Operation completed over 34 objects/3.2 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -n -r /home/sivan/whisper_base_fl_he_il gs://capstone_datasets/fleurs/finetune/wshiper_base_fl_he_il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}