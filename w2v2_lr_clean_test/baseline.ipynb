{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65fb5e6",
   "metadata": {},
   "source": [
    "### Baseline raw Wav2Vec2 WER on LibriSpeech clean-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becaf450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonin/capstone/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/antonin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from jiwer import wer\n",
    "import librosa\n",
    "import nltk\n",
    "import os\n",
    "import tarfile\n",
    "import torch\n",
    "import urllib.request\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912b6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "datasets_path = os.path.join(os.getcwd(), 'datasets') \n",
    "# create folders if they do not already exist\n",
    "if not os.path.exists(datasets_path): os.makedirs(datasets_path)\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0cb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_dataset_from_url(url: str, datasets_path: str = datasets_path):\n",
    "    \"\"\"\n",
    "    downloads and extracts dataset from url into datasets_path/\n",
    "    \"\"\"\n",
    "    temp = os.path.join(datasets_path, url.split('/')[-1])\n",
    "    print('downloading dataset...')\n",
    "    urllib.request.urlretrieve(url, temp)\n",
    "    print('extracting data...')\n",
    "    file = tarfile.open(temp)\n",
    "    file.extractall(datasets_path)\n",
    "    file.close()\n",
    "    os.remove(temp)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b69acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading dataset...\n",
      "extracting data...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "download_and_extract_dataset_from_url('https://www.openslr.org/resources/12/test-clean.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f069af",
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_eval = load_dataset(\"datasets/LibriSpeech\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1192229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav2vec_model(hf_path: str):\n",
    "    \"\"\"\n",
    "    load and return wav2vec tokenizer and model from huggingface\n",
    "    \"\"\"\n",
    "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(hf_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(hf_path)    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac57faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_text(batch):\n",
    "    \"\"\"\n",
    "    inserts ground truth in dataset\n",
    "    \"\"\"\n",
    "    transcription_file_path = batch['audio']['path'][:-10] + '.trans.txt'\n",
    "    f = open(transcription_file_path, 'r')\n",
    "    lines= str.splitlines(f.read())\n",
    "    txt=lines[int(batch['audio']['path'][-7:-5])].split(' ', 1)[1]\n",
    "    batch['txt'] = txt\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bd0da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [03:02<00:00, 14.32ex/s]\n"
     ]
    }
   ],
   "source": [
    "librispeech_eval = librispeech_eval.map(map_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d4e6201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/Users/antonin/capstone/lib/python3.9/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:754: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_wav2vec_model(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred(batch):\n",
    "    #tokenize\n",
    "    input_values = tokenizer(batch[\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "    #take logits\n",
    "    logits = model(input_values).logits\n",
    "    #take argmax (find most probable word id)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    #get the words from the predicted word ids\n",
    "    transcription = tokenizer.decode(predicted_ids[0])\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9de1341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [14:33<00:00,  3.00ex/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.0338557516737675\n"
     ]
    }
   ],
   "source": [
    "result = librispeech_eval.map(map_to_pred)\n",
    "\n",
    "print(\"WER:\", wer(result[\"txt\"], result[\"transcription\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b83e1c8d58c796f65d68445f553b9f4c555256228ca92f71880f99f509ba716c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
